{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# A Complete Solution to the BackBaze.com Kaggle Problem"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Step Three.  \n### Shaping and cleaning the data."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Table of contents"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "1. [Introduction](#10)<br>\n\n2. [Establish environment and parameters](#20)<br>\n3. [Data Shaping and Cleaning](#30)<br>\n    3.1 [Read in Data](#31)<br>\n    3.2 [Remove disks installed after to January 1st 2020 ](#32)<br>\n    3.3 [Remove disks that failed in January 2020 and data existing on disks after the failure date.](#33)<br>\n    3.4 [Ensure we are not missing any dates.](#34)<br>\n4. [Sample the data set to fit environment ](#40)<br>\n    4.1 [Understand the underlying proportion of failure to non-failure disks](#41)<br>\n    4.2 [Build the sample](#42)<br>\n5. [Write DF to a parque file for for use in step 3.](#50)<br>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 1.0 Introduction <a id=\"10\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Note this is part two of a four-part solution.\n\nBackBlaze.com, you are the \"GOAT.\" You are the \"cat's meow.\" You \"Rock the House.\" In case you don't know why BackBaze.com is so totally \"kick-ass,\" they open-sourced a vast set of hard drive information a few years ago and continue updating it each quarter.  What a treasure trove of superb data.  BackBlaze.com, thank you from the bottom of my heart.\n\nThe backblaze.com data includes operational metrics from hard drives with an indicator of a hard-drive failure.  It is an excellent source for teaching techniques related to machine failure.  Again, thank you for making this available to the open-source community.\nHere is a link to the data.\n\nhttps://www.backblaze.com/b2/hard-drive-test-data.html\n\nMy goal in this series of articles is not to give the best solution with the highest AUC.  My goal is to show you how to approach equipment failure problems and build solutions that reflect realistic accuracy, and provide an easy transition from the lab to the real world.\n\nI will use a Spark/Python Jupyter notebook inside IBM's Watson Studio on the cloud as a tool in this discussion.\n\nhttps://www.ibm.com/cloud/watson-studio\n\nI will also be using cloud object storage on the IBM cloud.\n\nhttps://www.ibm.com/cloud/block-storage\n\n\nThe third article in this series is not very exciting but is critical.  I will be shaping and scoping the data set I created in the first article, so it is ready for machine learning.\n\nRarely is it a good idea to take the data as it comes.  It is almost always a good idea to shape your data to be consistent with the problem that needs solving.  The backblaze.com data is no different.  In this notebook, we will take the raw data and shape it to pass it to a machine-learning model.\n\nIn the first article of this series, we read the raw data from 2018,2019, and 2020.  Data from 2017, 2018, and 2019 are for creating features.  The 2020 data is for modeling.  Therefore, our shaping exercise is limited to 2020 data.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 2.0 Establish environment and Parameters <a id=\"20\"></a>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Import the Relevant Libraries and connect to object storage."
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "#Import relevant Libraries\nfrom functools import reduce\nfrom pyspark.sql import DataFrame\n\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.functions import when"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 3.0 Data Shaping and Cleaning <a id=\"30\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### 3.1 Read in Data <a id=\"31\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Read in the data we created in the first notebook."
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "df_2020 = spark.read.parquet(cos.url('data2020.parquet', 'backblazedata-donotdelete-pr-cij57grgkoctem')).cache()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Note at the begining of the exercise we have 52,286,398 records."
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "#df_2020.count()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Check for duplicate records.  There are none."
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "#df_2020=df_2020.distinct()"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "#df_2020.count()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### 3.2 Remove disks installed after to January 1st 2020 <a id=\"32\"></a>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "When we deploy this model, we will do so on the existing set of disks at the current time.  I want the model to reflect that as well.  In other words, when we deploy this model, we will not predict disks installed in the future.  We will only know about the disks currently in the data center.  To approximate model deployment, it is essential to pick a point in time and only include the disk operating at that time.  In this case, our point in time is 1/1/2020. \n\nWe are defining our modeling data set to include all disk drives installed and operating on January 1, 2020.\n\nCreate a list of disk drives operating on 1/1/2020"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": "#Copy the data set so we can aggregate and transform it\ndf_start=df_2020\n#Keep Serial number and date.\ndf_start=df_start.select([c for c in df_start.columns if c in ['SERIAL_NUMBER','DATE']])\n#select records that existed on 1/1/2020\ndf_start=df_start.filter(df_start.DATE =='2020-01-01')\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Include the disks operating on 1/1/20 with an inner join on the original data frame.  The result is only disks running on 1/1/20."
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "df_start=df_start.select([c for c in df_start.columns if c in ['SERIAL_NUMBER']])\ndf_start = df_start.withColumnRenamed(\"SERIAL_NUMBER\",\"DOODEE\")\n\ndf_2020=df_2020.join(df_start,(((df_start.DOODEE) ==  (df_2020.SERIAL_NUMBER))),\"inner\")\ndf_2020 = df_2020.drop(\"DOODEE\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### 3.3 Remove disks that failed in January 2020 and data existing on disks after the failure date. <a id=\"33\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "It is essential to remove the disks that failed at the beginning of 2020.  \n\nWhy?  \n\nBecause these disks were on death's door before we even started, some, if not most, of the information that would predict failure for these disks doesn't exist in the data.  This information would be in 2019.\n\n\nAlso, we need to ensure that a disk does not linger in the data set after it fails.  If a disk fails on Monday, you don't want data for the disk on Tuesday or Wednesday. "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Convert Date to a date field."
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "df_2020=df_2020.withColumnRenamed(\"DATE\",\"INPUT\")\n\nfrom pyspark.sql.functions import *\n\ndf_2020=df_2020.select(to_date(col(\"INPUT\"),\"y-M-dd\").alias(\"DATE\"), \"*\")\ndf_2020 = df_2020.drop(\"INPUT\").cache()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create a data set that only includes the disks that failed."
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "#select records with a failure\ndf_get=df_2020.filter(df_2020.FAILURE ==1)\n\n#create a complete set of drives that failed.\ndf_get=df_get.select([c for c in df_get.columns if c in ['SERIAL_NUMBER']])\ndf_get=df_get.dropDuplicates(['SERIAL_NUMBER'])\n\ndf_get = df_get.withColumnRenamed(\"SERIAL_NUMBER\",\"DOODEE\")\ndf_failures=df_2020.join(df_get,(((df_get.DOODEE) ==  (df_2020.SERIAL_NUMBER))),\"inner\")\ndf_failures = df_failures.drop(\"DOODEE\")\n\n\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Capture the minimum failure date for each disk.  There may be situations where the same disk has two or more failure dates.  We want to identify the first."
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": "df_later=df_2020.filter(df_2020.FAILURE ==1)\n\nfrom pyspark.sql.functions import col, max as max_, min as min_\ndf=df_later\nv=(df.groupBy(\"SERIAL_NUMBER\")\n    .agg(min(\"DATE\")))\nv = v.withColumnRenamed(\"min(DATE)\",\"MINDATE\")\nv = v.withColumnRenamed(\"SERIAL_NUMBER\",\"DOODEE\")\n\nv=v.filter(v.MINDATE>='2020-01-30')\n\nv=v.sort((\"MINDATE\"))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Remove any records that exist after the first failure."
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": "df_failures=v.join(df_failures,(((v.DOODEE) ==  (df_failures.SERIAL_NUMBER))),\"inner\")\ndf_failures = df_failures.drop(\"DOODEE\")\n\n\ndf_failures=df_failures.filter(df_failures.DATE<=df_failures.MINDATE)\ndf_failures = df_failures.drop(\"DOODEE\")\ndf_failures = df_failures.drop(\"MINDATE\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create a dataframe that only includes disks that did not fail in 2020."
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": "df_bysn = df_2020.groupBy('SERIAL_NUMBER').agg(F.sum(\"FAILURE\").alias('sum_failure'))\ndf_bysn=df_bysn.filter(df_bysn.sum_failure==0)\n\ndf_bysn=df_bysn.select([c for c in df_bysn.columns if c in ['SERIAL_NUMBER']])\ndf_bysn=df_bysn.dropDuplicates(['SERIAL_NUMBER'])\n\ndf_bysn = df_bysn.withColumnRenamed(\"SERIAL_NUMBER\",\"DOODEE\")\n\ndf_nonfailures=df_2020.join(df_bysn,(((df_bysn.DOODEE) ==  (df_2020.SERIAL_NUMBER))),\"inner\")\ndf_nonfailures = df_nonfailures.drop(\"DOODEE\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Append the disks with failures and without failures into a single df"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": "\ndf_total = reduce(DataFrame.unionAll, [df_failures,df_nonfailures])\n\n#df_total.count()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now we have 43,844,671 records in the modeling dataframe."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### 3.4 Ensure we are not missing any dates. <a id=\"34\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "When working with time series or panel data, we need to ensure that all dates are consecutive.  That is, there is no missing time.  For example, if you have a time series measured daily for June, you must ensure that June 5th is not missing.  There are many ways to do this in pandas; unfortunately, I don't have an elegant way to do it in SPARK.  The following code is not elegant, but it works. "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create a field called \"ROW\" the represents the sequential value of each record for each disk."
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number\nwindowSpec  = Window.partitionBy(\"SERIAL_NUMBER\").orderBy(\"DATE\")\n\ndfzz1=df_total.withColumn(\"ROW\",row_number().over(windowSpec))\ndfzz=dfzz1"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create a DF with that adds 1 to the existing row number"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": "#select the relevant fields\ndfzz=dfzz.select([c for c in dfzz.columns if c in ['SERIAL_NUMBER', 'DATE','ROW']])\n#sort the data\ndfzz=dfzz.sort((['SERIAL_NUMBER','ROW']))\n#add one to row.\ndf_1x = dfzz.withColumn('ROW_1', ( dfzz['ROW'] + 1 ) )\n#get rid of ROW\ndf_1x = df_1x.drop(\"ROW\")\ndf_1=df_1x"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Rename the working data frame to become the date in the previous record, then merge it to the original data frame."
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": "\n#Rename the variables\ndf_1=df_1.withColumnRenamed(\"SERIAL_NUMBER\",\"SERIAL_NUMBER_1\")\ndf_1=df_1.withColumnRenamed(\"DATE\",\"DATE_1\")\n#merge to the original df\ndfzz=dfzz.join(df_1,(((dfzz.ROW) ==  (df_1.ROW_1)) & ((dfzz.SERIAL_NUMBER) ==  (df_1.SERIAL_NUMBER_1))),\"inner\")\n\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create a field called \"datediff\" that is the difference between the current date and the previous date."
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.sql.functions import *\ndfzz=dfzz.select(\n      col(\"SERIAL_NUMBER\"),\n      col(\"DATE\"),\n      col(\"DATE_1\"),\n      datediff(col(\"DATE\"),col(\"DATE_1\")).alias(\"datediff\"))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Take the maximum \"datediff\" for each disk."
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": "v=(dfzz.groupBy(\"SERIAL_NUMBER\")\n    .agg(max(\"datediff\")))\nv = v.withColumnRenamed(\"max(datediff)\",\"MAXDATEDIFF\")\nv = v.withColumnRenamed(\"SERIAL_NUMBER\",\"DOODEE\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Ensure that the maximum is 1, meaning the largest gap between any two consecutive days is 1.  The resulting data set will contain drives with no missing dates."
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": "v=v.filter(v.MAXDATEDIFF==1)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Join the data frame containing drives with no missing dates to the original data frame with an inner join.  This join eliminates disks with missing dates from the original data frame.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": "v=v.select([c for c in v.columns if c in ['DOODEE']])\n\ndf_total=df_total.join(v,(((df_total.SERIAL_NUMBER) ==  (v.DOODEE))),\"inner\")\ndf_total = df_total.drop(\"DOODEE\")"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": "#df_total.count()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now we have 42,202,057 where all disks are not missing consecutive dates."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 4.0 Sample the data set to fit environment <a id=\"40\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Unfortunately, money does not grow on trees.  I don't have unlimited resources to run this notebook and build a model.  In this section I down-sample the data so that I can run it in my environment."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### 4.1 Understand the underlying proportion of failure to non-failure disks. <a id=\"41\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "I will stratify the sample based on if a drive failed or did not fail in 2020.  The first step in doing so is to understand the underlying base incident failure rate among the disk drives."
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": "df_bysn = df_total.groupBy('SERIAL_NUMBER').agg(F.sum(\"FAILURE\").alias('sum_failure'))\ndf_nonfailurelist=df_bysn.filter(df_bysn.sum_failure==0)\ndf_failurelist=df_bysn.filter(df_bysn.sum_failure>0)\ndf_q = df_failurelist.unionByName(df_nonfailurelist)\n#df_q.groupBy(\"sum_failure\").count().show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We have 1084 disk drives that failed and 119,041 that did not fail.  That is a failure rate of about .9%.  Or, for every disk that failed, about 109 did not fail.  Your sample will need to fit your computing environment.  For me and my environment, I will select 1000 failures and 1000 non-failures.  Again, you will need to scale the sample to what you can actualy run in your computing environment."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### 4.2 Build the sample. <a id=\"42\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "First, randomly select 1000 disks with a failure."
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": "df_2020=df_total"
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": "#identify disks with a failure\ndf_failure=df_2020.filter(df_2020.FAILURE ==1)\n#keep relevant columns\ndf_failure=df_failure.select([c for c in df_failure.columns if c in ['SERIAL_NUMBER']])\n#dedup the data \ndf_failure=df_failure.dropDuplicates(['SERIAL_NUMBER'])\n#assign a random number to each serial number\ndf_failure=df_failure.withColumn('wookie', rand())\n#sort by the random number\ndf_failure=df_failure.sort((\"wookie\"))\n\n#keep the first 1000 records\ndf_failure=df_failure.limit(1000)\n\n#clean up the data\ndf_failure_list = df_failure.withColumnRenamed(\"SERIAL_NUMBER\",\"DOODEE\")\ndf_failure_list = df_failure_list.drop(\"wookie\")\n\n#conduct an inner join to the original data results set will be all records for the 1000 randomly selected disks\ndf_failure=df_2020.join(df_failure_list,(((df_failure_list.DOODEE) ==  (df_2020.SERIAL_NUMBER))),\"inner\")\ndf_failure = df_failure.drop(\"DOODEE\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Second, randomly select 1000 disks with a failure."
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": "#identify disks that did not fail\ndf_bysn = df_2020.groupBy('SERIAL_NUMBER').agg(F.sum(\"FAILURE\").alias('sum_failure'))\ndf_nonfailurelist=df_bysn.filter(df_bysn.sum_failure==0)\n#keep relevant variables\ndf_nonfailurelist=df_nonfailurelist.select([c for c in df_nonfailurelist.columns if c in ['SERIAL_NUMBER']])\n#ensure no duplicates\ndf_nonfailurelist=df_nonfailurelist.dropDuplicates(['SERIAL_NUMBER'])\n#create a random number for each disk\ndf_nonfailurelist=df_nonfailurelist.withColumn('wookie', rand())\n#sort the disks by the random number\ndf_nonfailurelist=df_nonfailurelist.sort((\"wookie\"))\n#keep 1000 randomly selected disks.\ndf_nonfailurelist=df_nonfailurelist.limit(1000)\n\n\n#clean up the data\ndf_nonfailurelist = df_nonfailurelist.withColumnRenamed(\"SERIAL_NUMBER\",\"DOODEE\")\ndf_nonfailurelist = df_nonfailurelist.drop(\"wookie\")\n#join to the original data.  The results set will be the all records from the randomly selected disks\ndf_nonfailures=df_2020.join(df_nonfailurelist,(((df_nonfailurelist.DOODEE) ==  (df_2020.SERIAL_NUMBER))),\"inner\")\ndf_nonfailures = df_nonfailures.drop(\"DOODEE\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Third, concentate the failures and non-failures"
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": "df_total = df_failure.unionByName(df_nonfailures)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### 5.0 Write DF to a parque file for for use in step 4. <a id=\"50\"></a>"
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": "\n\ndf_total.write.mode(\"overwrite\").parquet(cos.url('data_2020_final.parquet', 'backblazedata-donotdelete-pr-cij57grgkoctem'))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "All data used in this notebook is the property of BackBlaze.com. \n\nFor questions regarding use of data please see the following website.\nhttps://www.backblaze.com/b2/hard-drive-test-data.html"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.9 with Spark",
            "language": "python3",
            "name": "python39"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}