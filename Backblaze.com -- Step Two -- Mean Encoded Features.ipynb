{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# A Complete Solution to the BackBaze.com Kaggle Problem"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Step Two.  \n\n### Creating Mean Encoded Features"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Table of Contents"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "1. [Introduction](#10)<br>\n\n2. [Establish environment and parameters](#20)<br>\n3. [Read in data created in step 1](#30)<br>\n4. [Create mean encoded features](#40)<br>\n    4.1 [Establish a global mean](#41)<br>\n    4.2 [Aggregate the failure rate by Manufacturer](#42)<br>\n    4.3 [Aggregate the failure rate by Model](#43)<br>\n    4.4 [Calculate the values of predictors when a disk fails by model](#44)<br>\n    4.5 [Calculate the values of predictors when a disk fails by manufacturer](#45)<br>\n\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 1.0 Introduction <a id=\"10\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "BackBaze.com, you are the \"GOAT.\" You are the \"cat's meow.\" You \"Rock the House.\" In case you don't know why BackBaze.com is so totally \"kick-ass,\" they open-sourced a vast set of hard drive information a few years ago and continue updating it each quarter. What a treasure trove of superb data. BackBlaze.com, thank you from the bottom of my heart. \n\nThe backblaze.com data includes operational metrics from hard drives with an indicator of a hard-drive failure. It is an excellent source for teaching techniques related to machine failure. Again, thank you for making this available to the open-source community.\n\nHere is a link to the data.\n\n\nhttps://www.backblaze.com/b2/hard-drive-test-data.html\n\nMy goal in this series of articles is not to give the best solution with the highest AUC.  My goal is to show you how to approach equipment failure problems and build solutions that reflect realistic accuracy, and provide an easy transition from the lab to the real world. \n\nI will use a Spark/Python Jupyter notebook inside IBM's Watson Studio on the cloud as a tool in this discussion.\n\nhttps://www.ibm.com/cloud/watson-studio\n\nI will also be using cloud object storage on the IBM cloud.\n\nhttps://www.ibm.com/cloud/block-storage\n\n\nThe third article is about designing features for a predictive model.  Specifically, using data from 2017, 2018 and 2019 to build features for our model based on 2020.  For more information on mean encoded features, please see the following article.\n\nhttps://medium.com/towards-data-science/leveraging-value-from-postal-codes-naics-codes-area-codes-and-other-funky-arse-categorical-be9ce75b6d5a\n\nI created these notebooks with a runtime useing 1 driver with 1 vCPU and 4 GB RAM, and 2 executors each with 1 vCPU and 4 GB RAM. This is available for free on the IBM Cloud. Some of the notebooks take a few hours to run. You'll need to schedule your notebooks to run as jobs.\n\nhttps://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/schedule-task.html"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 2.0 Establish environment and parameters <a id=\"20\"></a>"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "from functools import reduce\nfrom pyspark.sql import DataFrame\n\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.functions import when\n\nfrom pyspark.sql.functions import rand\nfrom pyspark.sql.functions import lit"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 3.0 Read in data created in step 1 <a id=\"30\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Read in the data from 2017-2019 we created in step 1.  There are 10,7909,839 observations and 16 fields."
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "df = spark.read.parquet(cos.url('data2019_2017.parquet', 'backblazedata-donotdelete-pr-cij57grgkoctem'))\n\n#df=df.show(200)\n#print((df.count(), len(df.columns)))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 4.0 Create mean encoded features. <a id=\"40\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In the next few steps we will created mean encoded features and export to a format to be used in our predictive model.  \n\nOne thing you want to avoid when building mean encoded features is using irrelevant aggregations.  This can occur when you don't have a big enough sample for the aggregation to be meaningful.  The failure rate is tiny in this exercise.  This means we must ensure a large number of observations are available for the aggregations to be meaningful.  In the code below, I use 10,000 as a threshold.  There is no magic number, but I picked 10,000 based on the fact that overall average failure rate across all disk drives."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### 4.1 Establish a global mean <a id=\"41\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Having a global mean in the data frame allows us to easily replace irrelevant values with an average."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create a Dummy field to use in aggregations."
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "df = df.withColumn(\"wookie\", lit(1))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create a global failure rate accross all three years of data.  We will use this when we create the features.  We now have a spark dataframe that expresses the average failure rate across all disks for the last three years."
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "#aggregate the data\ntotal = df.groupBy('wookie').agg(F.mean(\"failure\").alias('avg_failure')).collect()\n#convert output to rdd\nrdd = spark.sparkContext.parallelize(total)\n#convert output to spark\nzz=rdd.toDF()\n#rename the column\nzz=zz.withColumnRenamed(\"avg_failure\",\"GLOBAL_AVG_FAILURE\")\n#multiply by 10,000, for formatting purposes.\nzz = zz.withColumn(\"GLOBAL_AVG_FAILURE\", zz.GLOBAL_AVG_FAILURE*10000)\n#zz.show(200)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n\n\n\n\n\n\nJoin the global average to the original data frame."
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "df=df.join(zz,(df.wookie) ==  (zz.wookie),\"inner\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### 4.2 Aggregate the failure rate by Manufacturer. <a id=\"42\"></a>"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Calculate the summaries.\ntotal = df.groupBy('MANUFACTURER').agg(F.mean(\"failure\").alias('avg_failure'),F.count(\"failure\").alias('count_failure'),\\\n                                       F.sum(\"failure\").alias('sum_failure'),F.mean(\"GLOBAL_AVG_FAILURE\").alias('GLOBAL_AVG_FAILURE')).collect()\n#Convert to RDD\nrdd = spark.sparkContext.parallelize(total)\n#convert to spark\nzz=rdd.toDF()\n#rename columns\nzz=zz.withColumnRenamed(\"avg_failure\",\"MANU_FAIL_RATE\")\nzz=zz.withColumnRenamed(\"sum_failure\",\"MANU_FAIL_TOTAL\")\nzz=zz.withColumnRenamed(\"count_failure\",\"MANU_FAIL_CNT\")\n#multiply by 10,000 to make them easier to read and deal with\nzz = zz.withColumn(\"MANU_FAIL_RATE\", zz.MANU_FAIL_RATE*10000)\n\n#zz.show(200)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We want to avoid situations where aggregations are based on a small number of records.  In the next step we replace values with the global failure average if the total number of records used to calcluate the value is less than 10,000.  Again, 10,000 is reasonable based on the overall failure rate."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df_manu = zz.withColumn(\"MANU_FAIL_RATE\", when(zz.MANU_FAIL_CNT<100000,zz.GLOBAL_AVG_FAILURE).otherwise(zz.MANU_FAIL_RATE))\n\n#df_manu.show(200)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Convert the aggregated data frame to pandas."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df_manup = df_manu.toPandas()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Define credentials for object storage"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from ibm_botocore.client import Config\nimport ibm_boto3\ncos = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n    config=Config(signature_version='oauth'),\n    endpoint_url=credentials['ENDPOINT'])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Export the pandas dataframe to csv and upload to cloud object storage."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df_manup=df_manup.to_csv('manufacturer.csv',index=False)\ncos.upload_file(Filename='manufacturer.csv',Bucket=credentials['BUCKET'],Key='manufacturer.csv')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### 4.3 Aggregate the failure rate by Model. <a id=\"43\"></a>"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Calculate the summaries.\ntotal = df.groupBy('MODEL').agg(F.mean(\"failure\").alias('avg_failure'),F.count(\"failure\").alias('count_failure'),\\\n                                       F.sum(\"failure\").alias('sum_failure'),F.mean(\"GLOBAL_AVG_FAILURE\").alias('GLOBAL_AVG_FAILURE')).collect()\n#Convert to RDD\nrdd = spark.sparkContext.parallelize(total)\n#convert output to spark\n\nzz=rdd.toDF()\n#rename columns\nzz=zz.withColumnRenamed(\"avg_failure\",\"MODEL_FAIL_RATE\")\nzz=zz.withColumnRenamed(\"sum_failure\",\"MODEL_FAIL_TOTAL\")\nzz=zz.withColumnRenamed(\"count_failure\",\"MODEL_FAIL_CNT\")\n#multiply by 10,000 to make them easier to read and deal with\nzz = zz.withColumn(\"MODEL_FAIL_RATE\", zz.MODEL_FAIL_RATE*10000)\n\n#replace values when total for a summary is less than 10,000\ndf_model = zz.withColumn(\"MODEL_FAIL_RATE\", when(zz.MODEL_FAIL_CNT<100000,zz.GLOBAL_AVG_FAILURE).otherwise(zz.MODEL_FAIL_RATE))\n\n#convert to Pandas\ndf_modelp = df_model.toPandas()\n#export to csv\ndf_modelp=df_modelp.to_csv('model.csv',index=False)\n#upload to object storage\ncos.upload_file(Filename='model.csv',Bucket=credentials['BUCKET'],Key='model.csv')\n#zz.show(200)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### 4.4 Calculate the values of predictors when a disk fails by model. <a id=\"44\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "This aggregation could be a useful predictor when compared to other non-failure values.  For example, if a disk fails everytime a field is equal to 76.4, you should probably take note.\n\nSelect disks that failed"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df_failure=df.filter(df.FAILURE == 1)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Aggregate the fields by model"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Calculate the summaries.\ntotal = df_failure.groupBy('MODEL').agg(F.mean(\"REAllOCATED_SECTOR_COUNT_N\").alias('REALLOCATED_SECTOR_COUNT_N_MOD'),\\\n                                F.mean(\"REPORTED_UNCORRECTABLE_ERRORS_N\").alias('REPORTED_UNCORRECTABLE_ERRORS_N_MOD'),\\\n                                F.mean(\"COMMAND_TIMEOUT_N\").alias('COMMAND_TIMEOUT_N_MOD'),\\\n                                F.mean(\"CURRENT_PENDING_SECTOR_COUNT_N\").alias('CURRENT_PENDING_SECTOR_COUNT_N_MOD'),\\\n                                F.mean(\"POWER_ON_HOURS_N\").alias('POWER_ON_HOURS_N_MOD'),\\\n                                F.mean(\"REAllOCATED_SECTOR_COUNT_R\").alias('REALLOCATED_SECTOR_COUNT_R_MOD'),\\\n                                F.mean(\"REPORTED_UNCORRECTABLE_ERRORS_R\").alias('REPORTED_UNCORRECTABLE_ERRORS_R_MOD'),\\\n                                F.mean(\"COMMAND_TIMEOUT_R\").alias('COMMAND_TIMEOUT_R_MOD'),\\\n                                F.mean(\"CURRENT_PENDING_SECTOR_COUNT_R\").alias('CURRENT_PENDING_SECTOR_COUNT_R_MOD'),\\\n                                F.mean(\"POWER_ON_HOURS_R\").alias('POWER_ON_HOURS_R_MOD')).collect()\n#Convert to RDD\nrdd = spark.sparkContext.parallelize(total)\n\n#convert to spark\ndf_avg_by_model=rdd.toDF()\n#convert to pandas\ndf_avg_by_model = df_avg_by_model.toPandas()\n#export to csv\ndf_avg_by_model=df_avg_by_model.to_csv('df_avg_by_model.csv',index=False)\n#upload to cloud object storage\ncos.upload_file(Filename='df_avg_by_model.csv',Bucket=credentials['BUCKET'],Key='df_avg_by_model.csv')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### 4.5 Calculate the values of predictors when a disk fails by manufacturer. <a id=\"45\"></a>"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Calculate the summaries.\ntotal = df_failure.groupBy('MANUFACTURER').agg(F.mean(\"REAllOCATED_SECTOR_COUNT_N\").alias('REALLOCATED_SECTOR_COUNT_N_MAN'),\\\n                                F.mean(\"REPORTED_UNCORRECTABLE_ERRORS_N\").alias('REPORTED_UNCORRECTABLE_ERRORS_N_MAN'),\\\n                                F.mean(\"COMMAND_TIMEOUT_N\").alias('COMMAND_TIMEOUT_N_MAN'),\\\n                                F.mean(\"CURRENT_PENDING_SECTOR_COUNT_N\").alias('CURRENT_PENDING_SECTOR_COUNT_N_MAN'),\\\n                                F.mean(\"POWER_ON_HOURS_N\").alias('POWER_ON_HOURS_N_MAN'),\\\n                                F.mean(\"REAllOCATED_SECTOR_COUNT_R\").alias('REALLOCATED_SECTOR_COUNT_R_MAN'),\\\n                                F.mean(\"REPORTED_UNCORRECTABLE_ERRORS_R\").alias('REPORTED_UNCORRECTABLE_ERRORS_R_MAN'),\\\n                                F.mean(\"COMMAND_TIMEOUT_R\").alias('COMMAND_TIMEOUT_R_MAN'),\\\n                                F.mean(\"CURRENT_PENDING_SECTOR_COUNT_R\").alias('CURRENT_PENDING_SECTOR_COUNT_R_MAN'),\\\n                                F.mean(\"POWER_ON_HOURS_R\").alias('POWER_ON_HOURS_R_MAN')).collect()\n#Convert to RDD\nrdd = spark.sparkContext.parallelize(total)\n\n#convert to spark\ndf_avg_by_manu=rdd.toDF()\n\n#convert to pandas\ndf_avg_by_manu = df_avg_by_manu.toPandas()\n#export to csv\ndf_avg_by_manu=df_avg_by_manu.to_csv('df_avg_by_manu.csv',index=False)\n#upload to cloud object storage\ncos.upload_file(Filename='df_avg_by_manu.csv',Bucket=credentials['BUCKET'],Key='df_avg_by_manu.csv')\n\n#df_avg_by_manu.show(10)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "All data used in this notebook is the property of BackBlaze.com.\n\nFor questions regarding use of data please see the following website. https://www.backblaze.com/b2/hard-drive-test-data.html"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.9 with Spark",
            "language": "python3",
            "name": "python39"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}