{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# A Complete Solution to the BackBaze.com Kaggle Problem"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Step Four.  Adding features to the data"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Table of Contents"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "1. [Introduction](#10)<br>\n\n2. [Establish environment and parameters](#20)<br>\n3. [Create lagged features](#30)<br>\n4. [Create Max, Min, Sum, Mean and Relative Change Features](#40)<br>\n5. [Append mean encoded data variables](#50)<br>\n6.  [Create even more features](#60)<br>\n7. [Export to Parquet file for step 5](#70)<br>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 1.0 Introduction <a id=\"10\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Note this is part two of a four-part solution.\n\nBackBlaze.com, you are the \"GOAT.\" You are the \"cat's meow.\" You \"Rock the House.\" In case you don't know why BackBaze.com is so totally \"kick-ass,\" they open-sourced a vast set of hard drive information a few years ago and continue updating it each quarter.  What a treasure trove of superb data.  BackBlaze.com, thank you from the bottom of my heart.\n\nThe backblaze.com data includes operational metrics from hard drives with an indicator of a hard-drive failure.  It is an excellent source for teaching techniques related to machine failure.  Again, thank you for making this available to the open-source community.\nHere is a link to the data.\n\nhttps://www.backblaze.com/b2/hard-drive-test-data.html\n\nMy goal in this series of articles is not to give the best solution with the highest AUC.  My goal is to show you how to approach equipment failure problems and build solutions that reflect realistic accuracy, and provide an easy transition from the lab to the real world.\n\nI will use a Spark/Python Jupyter notebook inside IBM's Watson Studio on the cloud as a tool in this discussion.\n\nhttps://www.ibm.com/cloud/watson-studio\n\nI will also be using cloud object storage on the IBM cloud.\n\nhttps://www.ibm.com/cloud/block-storage\n\n\nThe fourth article in this series we will create featues and append them to the data.  We will also append the features we created in Step Two. \n\nI created these notebooks with a runtime useing 1 driver with 1 vCPU and 4 GB RAM, and 2 executors each with 1 vCPU and 4 GB RAM. This is available for free on the IBM Cloud. Some of the notebooks take a few hours to run. You'll need to schedule your notebooks to run as jobs.\n\nhttps://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/schedule-task.html"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 2.0 Establish environment and parameters <a id=\"20\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Import the Relevant Libraries, connect to object storage and import data from previous step."
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "from functools import reduce\nfrom pyspark.sql import DataFrame\n\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import *\n\nfrom pyspark.sql.functions import when\n\nfrom pyspark.sql.functions import rand\nfrom pyspark.sql.functions import lit\n\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.sql.functions import col, round\n\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number\n\n\nimport pandas as pd\n\n\nspark.conf.set(\"spark.sql.broadcastTimeout\",  7200000)\nspark.conf.set(\"spark.sql.parquet.compression.codec\", \"gzip\")\n"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "df = spark.read.parquet(cos.url('data_2020_final.parquet', 'backblazedata-donotdelete-pr-cij57grgkoctem'))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Reformat numeric values to double"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "for c in [ 'REALLOCATED_SECTOR_COUNT_N',\n 'REPORTED_UNCORRECTABLE_ERRORS_N',\n 'COMMAND_TIMEOUT_N',\n 'CURRENT_PENDING_SECTOR_COUNT_N',\n 'POWER_ON_HOURS_N',\n 'REALLOCATED_SECTOR_COUNT_R',\n 'REPORTED_UNCORRECTABLE_ERRORS_R',\n 'COMMAND_TIMEOUT_R',\n 'CURRENT_PENDING_SECTOR_COUNT_R',\n 'POWER_ON_HOURS_R','FAILURE','CAPACITY_BYTES']:\n    # add condition for the cols to be type cast\n    df=df.withColumn(c, df[c].cast('double'))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 3.0 Create lagged features  <a id=\"30\"></a>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create a consecutive row number for each record and serial number."
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "\ndf=df.sort(\"SERIAL_NUMBER\", \"DATE\")\nwindowSpec  = Window.partitionBy(\"SERIAL_NUMBER\").orderBy(\"DATE\")\n\ndf=df.withColumn(\"ROW\",row_number().over(windowSpec))"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "dfx=df"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create lagged by 7 features"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": "\ndf_7 = dfx.withColumn('ROW_7', ( dfx['ROW'] + 7 ) )\ndf_7 = df_7.drop(\"ROW\")\ndf_7 = df_7.drop(\"DATE\")\ndf_7 = df_7.drop(\"MODEL\")\ndf_7 = df_7.drop(\"MANUFACTURER\")\ndf_7 = df_7.drop(\"CAPACITY_BYTES\")\ndf_7 = df_7.drop(\"FAILURE\")\n\ndf_7=df_7.withColumnRenamed(\"REALLOCATED_SECTOR_COUNT_R\",\"REALLOCATED_SECTOR_COUNT_R_7\")\ndf_7=df_7.withColumnRenamed(\"SERIAL_NUMBER\",\"SERIAL_NUMBER_7\")\ndf_7=df_7.withColumnRenamed(\"REALLOCATED_SECTOR_COUNT_N\",\"REALLOCATED_SECTOR_COUNT_N_7\")\ndf_7=df_7.withColumnRenamed(\"REPORTED_UNCORRECTABLE_ERRORS_N\",\"REPORTED_UNCORRECTABLE_ERRORS_N_7\")\ndf_7=df_7.withColumnRenamed(\"COMMAND_TIMEOUT_N\",\"COMMAND_TIMEOUT_N_7\")\ndf_7=df_7.withColumnRenamed(\"CURRENT_PENDING_SECTOR_COUNT_N\",\"CURRENT_PENDING_SECTOR_COUNT_N_7\")\ndf_7=df_7.withColumnRenamed(\"POWER_ON_HOURS_N\",\"POWER_ON_HOURS_N_7\")\ndf_7=df_7.withColumnRenamed(\"REPORTED_UNCORRECTABLE_ERRORS_R\",\"REPORTED_UNCORRECTABLE_ERRORS_R_7\")\ndf_7=df_7.withColumnRenamed(\"COMMAND_TIMEOUT_R\",\"COMMAND_TIMEOUT_R_7\")\ndf_7=df_7.withColumnRenamed(\"CURRENT_PENDING_SECTOR_COUNT_R\",\"CURRENT_PENDING_SECTOR_COUNT_R_7\")\ndf_7=df_7.withColumnRenamed(\"POWER_ON_HOURS_R\",\"POWER_ON_HOURS_R_7\")\n\n\n\ndf=df.join(df_7,(((df.ROW) ==  (df_7.ROW_7)) & ((df.SERIAL_NUMBER) ==  (df_7.SERIAL_NUMBER_7))),\"left\")\ndf = df.drop(\"SERIAL_NUMBER_7\")\ndf = df.drop(\"ROW_7\")\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create lagged by 6 features"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "\ndf_6 = dfx.withColumn('ROW_6', ( dfx['ROW'] + 6 ) )\ndf_6 = df_6.drop(\"ROW\")\ndf_6 = df_6.drop(\"DATE\")\ndf_6 = df_6.drop(\"MODEL\")\ndf_6 = df_6.drop(\"MANUFACTURER\")\n\ndf_6 = df_6.drop(\"CAPACITY_BYTES\")\ndf_6 = df_6.drop(\"FAILURE\")\n\n\ndf_6=df_6.withColumnRenamed(\"REALLOCATED_SECTOR_COUNT_R\",\"REALLOCATED_SECTOR_COUNT_R_6\")\ndf_6=df_6.withColumnRenamed(\"SERIAL_NUMBER\",\"SERIAL_NUMBER_6\")\ndf_6=df_6.withColumnRenamed(\"REALLOCATED_SECTOR_COUNT_N\",\"REALLOCATED_SECTOR_COUNT_N_6\")\ndf_6=df_6.withColumnRenamed(\"REPORTED_UNCORRECTABLE_ERRORS_N\",\"REPORTED_UNCORRECTABLE_ERRORS_N_6\")\ndf_6=df_6.withColumnRenamed(\"COMMAND_TIMEOUT_N\",\"COMMAND_TIMEOUT_N_6\")\ndf_6=df_6.withColumnRenamed(\"CURRENT_PENDING_SECTOR_COUNT_N\",\"CURRENT_PENDING_SECTOR_COUNT_N_6\")\ndf_6=df_6.withColumnRenamed(\"POWER_ON_HOURS_N\",\"POWER_ON_HOURS_N_6\")\ndf_6=df_6.withColumnRenamed(\"REPORTED_UNCORRECTABLE_ERRORS_R\",\"REPORTED_UNCORRECTABLE_ERRORS_R_6\")\ndf_6=df_6.withColumnRenamed(\"COMMAND_TIMEOUT_R\",\"COMMAND_TIMEOUT_R_6\")\ndf_6=df_6.withColumnRenamed(\"CURRENT_PENDING_SECTOR_COUNT_R\",\"CURRENT_PENDING_SECTOR_COUNT_R_6\")\ndf_6=df_6.withColumnRenamed(\"POWER_ON_HOURS_R\",\"POWER_ON_HOURS_R_6\")\n\ndf=df.join(df_6,(((df.ROW) ==  (df_6.ROW_6)) & ((df.SERIAL_NUMBER) ==  (df_6.SERIAL_NUMBER_6))),\"left\")\ndf = df.drop(\"SERIAL_NUMBER_6\")\ndf = df.drop(\"ROW_6\")\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create lagged by 5 features"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "\ndf_5 = dfx.withColumn('ROW_5', ( dfx['ROW'] + 5 ) )\ndf_5 = df_5.drop(\"ROW\")\ndf_5 = df_5.drop(\"DATE\")\ndf_5 = df_5.drop(\"MODEL\")\ndf_5 = df_5.drop(\"MANUFACTURER\")\n\ndf_5 = df_5.drop(\"CAPACITY_BYTES\")\ndf_5 = df_5.drop(\"FAILURE\")\ndf_5=df_5.withColumnRenamed(\"REALLOCATED_SECTOR_COUNT_R\",\"REALLOCATED_SECTOR_COUNT_R_5\")\ndf_5=df_5.withColumnRenamed(\"SERIAL_NUMBER\",\"SERIAL_NUMBER_5\")\ndf_5=df_5.withColumnRenamed(\"REALLOCATED_SECTOR_COUNT_N\",\"REALLOCATED_SECTOR_COUNT_N_5\")\ndf_5=df_5.withColumnRenamed(\"REPORTED_UNCORRECTABLE_ERRORS_N\",\"REPORTED_UNCORRECTABLE_ERRORS_N_5\")\ndf_5=df_5.withColumnRenamed(\"COMMAND_TIMEOUT_N\",\"COMMAND_TIMEOUT_N_5\")\ndf_5=df_5.withColumnRenamed(\"CURRENT_PENDING_SECTOR_COUNT_N\",\"CURRENT_PENDING_SECTOR_COUNT_N_5\")\ndf_5=df_5.withColumnRenamed(\"POWER_ON_HOURS_N\",\"POWER_ON_HOURS_N_5\")\ndf_5=df_5.withColumnRenamed(\"REPORTED_UNCORRECTABLE_ERRORS_R\",\"REPORTED_UNCORRECTABLE_ERRORS_R_5\")\ndf_5=df_5.withColumnRenamed(\"COMMAND_TIMEOUT_R\",\"COMMAND_TIMEOUT_R_5\")\ndf_5=df_5.withColumnRenamed(\"CURRENT_PENDING_SECTOR_COUNT_R\",\"CURRENT_PENDING_SECTOR_COUNT_R_5\")\ndf_5=df_5.withColumnRenamed(\"POWER_ON_HOURS_R\",\"POWER_ON_HOURS_R_5\")\n\ndf=df.join(df_5,(((df.ROW) ==  (df_5.ROW_5)) & ((df.SERIAL_NUMBER) ==  (df_5.SERIAL_NUMBER_5))),\"left\")\ndf = df.drop(\"SERIAL_NUMBER_5\")\ndf = df.drop(\"ROW_5\")\n\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create lagged by 4 features"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "\ndf_4 = dfx.withColumn('ROW_4', ( dfx['ROW'] + 4 ) )\ndf_4 = df_4.drop(\"ROW\")\ndf_4 = df_4.drop(\"DATE\")\ndf_4 = df_4.drop(\"MODEL\")\ndf_4 = df_4.drop(\"MANUFACTURER\")\n\ndf_4 = df_4.drop(\"CAPACITY_BYTES\")\ndf_4 = df_4.drop(\"FAILURE\")\ndf_4=df_4.withColumnRenamed(\"REALLOCATED_SECTOR_COUNT_R\",\"REALLOCATED_SECTOR_COUNT_R_4\")\ndf_4=df_4.withColumnRenamed(\"SERIAL_NUMBER\",\"SERIAL_NUMBER_4\")\ndf_4=df_4.withColumnRenamed(\"REALLOCATED_SECTOR_COUNT_N\",\"REALLOCATED_SECTOR_COUNT_N_4\")\ndf_4=df_4.withColumnRenamed(\"REPORTED_UNCORRECTABLE_ERRORS_N\",\"REPORTED_UNCORRECTABLE_ERRORS_N_4\")\ndf_4=df_4.withColumnRenamed(\"COMMAND_TIMEOUT_N\",\"COMMAND_TIMEOUT_N_4\")\ndf_4=df_4.withColumnRenamed(\"CURRENT_PENDING_SECTOR_COUNT_N\",\"CURRENT_PENDING_SECTOR_COUNT_N_4\")\ndf_4=df_4.withColumnRenamed(\"POWER_ON_HOURS_N\",\"POWER_ON_HOURS_N_4\")\ndf_4=df_4.withColumnRenamed(\"REPORTED_UNCORRECTABLE_ERRORS_R\",\"REPORTED_UNCORRECTABLE_ERRORS_R_4\")\ndf_4=df_4.withColumnRenamed(\"COMMAND_TIMEOUT_R\",\"COMMAND_TIMEOUT_R_4\")\ndf_4=df_4.withColumnRenamed(\"CURRENT_PENDING_SECTOR_COUNT_R\",\"CURRENT_PENDING_SECTOR_COUNT_R_4\")\ndf_4=df_4.withColumnRenamed(\"POWER_ON_HOURS_R\",\"POWER_ON_HOURS_R_4\")\n\ndf=df.join(df_4,(((df.ROW) ==  (df_4.ROW_4)) & ((df.SERIAL_NUMBER) ==  (df_4.SERIAL_NUMBER_4))),\"left\")\ndf = df.drop(\"SERIAL_NUMBER_4\")\ndf = df.drop(\"ROW_4\")\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create lagged by 3 features"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": "df_3 = dfx.withColumn('ROW_3', ( dfx['ROW'] + 3 ) )\ndf_3 = df_3.drop(\"ROW\")\ndf_3 = df_3.drop(\"DATE\")\ndf_3 = df_3.drop(\"MODEL\")\ndf_3 = df_3.drop(\"MANUFACTURER\")\ndf_3 = df_3.drop(\"CAPACITY_BYTES\")\ndf_3 = df_3.drop(\"FAILURE\")\n\ndf_3=df_3.withColumnRenamed(\"REALLOCATED_SECTOR_COUNT_R\",\"REALLOCATED_SECTOR_COUNT_R_3\")\ndf_3=df_3.withColumnRenamed(\"SERIAL_NUMBER\",\"SERIAL_NUMBER_3\")\ndf_3=df_3.withColumnRenamed(\"REALLOCATED_SECTOR_COUNT_N\",\"REALLOCATED_SECTOR_COUNT_N_3\")\ndf_3=df_3.withColumnRenamed(\"REPORTED_UNCORRECTABLE_ERRORS_N\",\"REPORTED_UNCORRECTABLE_ERRORS_N_3\")\ndf_3=df_3.withColumnRenamed(\"COMMAND_TIMEOUT_N\",\"COMMAND_TIMEOUT_N_3\")\ndf_3=df_3.withColumnRenamed(\"CURRENT_PENDING_SECTOR_COUNT_N\",\"CURRENT_PENDING_SECTOR_COUNT_N_3\")\ndf_3=df_3.withColumnRenamed(\"POWER_ON_HOURS_N\",\"POWER_ON_HOURS_N_3\")\ndf_3=df_3.withColumnRenamed(\"REPORTED_UNCORRECTABLE_ERRORS_R\",\"REPORTED_UNCORRECTABLE_ERRORS_R_3\")\ndf_3=df_3.withColumnRenamed(\"COMMAND_TIMEOUT_R\",\"COMMAND_TIMEOUT_R_3\")\ndf_3=df_3.withColumnRenamed(\"CURRENT_PENDING_SECTOR_COUNT_R\",\"CURRENT_PENDING_SECTOR_COUNT_R_3\")\ndf_3=df_3.withColumnRenamed(\"POWER_ON_HOURS_R\",\"POWER_ON_HOURS_R_3\")\n\ndf=df.join(df_3,(((df.ROW) ==  (df_3.ROW_3)) & ((df.SERIAL_NUMBER) ==  (df_3.SERIAL_NUMBER_3))),\"left\")\ndf = df.drop(\"SERIAL_NUMBER_3\")\ndf = df.drop(\"ROW_3\")\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create lagged by 2 features"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": "df_2 = dfx.withColumn('ROW_2', ( dfx['ROW'] + 2 ) )\ndf_2 = df_2.drop(\"ROW\")\ndf_2 = df_2.drop(\"DATE\")\ndf_2 = df_2.drop(\"MODEL\")\ndf_2 = df_2.drop(\"MANUFACTURER\")\ndf_2 = df_2.drop(\"CAPACITY_BYTES\")\ndf_2 = df_2.drop(\"FAILURE\")\ndf_2=df_2.withColumnRenamed(\"REALLOCATED_SECTOR_COUNT_R\",\"REALLOCATED_SECTOR_COUNT_R_2\")\ndf_2=df_2.withColumnRenamed(\"SERIAL_NUMBER\",\"SERIAL_NUMBER_2\")\ndf_2=df_2.withColumnRenamed(\"REALLOCATED_SECTOR_COUNT_N\",\"REALLOCATED_SECTOR_COUNT_N_2\")\ndf_2=df_2.withColumnRenamed(\"REPORTED_UNCORRECTABLE_ERRORS_N\",\"REPORTED_UNCORRECTABLE_ERRORS_N_2\")\ndf_2=df_2.withColumnRenamed(\"COMMAND_TIMEOUT_N\",\"COMMAND_TIMEOUT_N_2\")\ndf_2=df_2.withColumnRenamed(\"CURRENT_PENDING_SECTOR_COUNT_N\",\"CURRENT_PENDING_SECTOR_COUNT_N_2\")\ndf_2=df_2.withColumnRenamed(\"POWER_ON_HOURS_N\",\"POWER_ON_HOURS_N_2\")\ndf_2=df_2.withColumnRenamed(\"REPORTED_UNCORRECTABLE_ERRORS_R\",\"REPORTED_UNCORRECTABLE_ERRORS_R_2\")\ndf_2=df_2.withColumnRenamed(\"COMMAND_TIMEOUT_R\",\"COMMAND_TIMEOUT_R_2\")\ndf_2=df_2.withColumnRenamed(\"CURRENT_PENDING_SECTOR_COUNT_R\",\"CURRENT_PENDING_SECTOR_COUNT_R_2\")\ndf_2=df_2.withColumnRenamed(\"POWER_ON_HOURS_R\",\"POWER_ON_HOURS_R_2\")\n\ndf=df.join(df_2,(((df.ROW) ==  (df_2.ROW_2)) & ((df.SERIAL_NUMBER) ==  (df_2.SERIAL_NUMBER_2))),\"left\")\ndf = df.drop(\"SERIAL_NUMBER_2\")\ndf = df.drop(\"ROW_2\")\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create lagged by 1 features"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": "df_1 = dfx.withColumn('ROW_1', ( dfx['ROW'] + 1 ) )\ndf_1 = df_1.drop(\"ROW\")\ndf_1 = df_1.drop(\"DATE\")\ndf_1 = df_1.drop(\"MODEL\")\ndf_1 = df_1.drop(\"MANUFACTURER\")\ndf_1 = df_1.drop(\"CAPACITY_BYTES\")\ndf_1 = df_1.drop(\"FAILURE\")\ndf_1=df_1.withColumnRenamed(\"REALLOCATED_SECTOR_COUNT_R\",\"REALLOCATED_SECTOR_COUNT_R_1\")\ndf_1=df_1.withColumnRenamed(\"SERIAL_NUMBER\",\"SERIAL_NUMBER_1\")\ndf_1=df_1.withColumnRenamed(\"REALLOCATED_SECTOR_COUNT_N\",\"REALLOCATED_SECTOR_COUNT_N_1\")\ndf_1=df_1.withColumnRenamed(\"REPORTED_UNCORRECTABLE_ERRORS_N\",\"REPORTED_UNCORRECTABLE_ERRORS_N_1\")\ndf_1=df_1.withColumnRenamed(\"COMMAND_TIMEOUT_N\",\"COMMAND_TIMEOUT_N_1\")\ndf_1=df_1.withColumnRenamed(\"CURRENT_PENDING_SECTOR_COUNT_N\",\"CURRENT_PENDING_SECTOR_COUNT_N_1\")\ndf_1=df_1.withColumnRenamed(\"POWER_ON_HOURS_N\",\"POWER_ON_HOURS_N_1\")\ndf_1=df_1.withColumnRenamed(\"REPORTED_UNCORRECTABLE_ERRORS_R\",\"REPORTED_UNCORRECTABLE_ERRORS_R_1\")\ndf_1=df_1.withColumnRenamed(\"COMMAND_TIMEOUT_R\",\"COMMAND_TIMEOUT_R_1\")\ndf_1=df_1.withColumnRenamed(\"CURRENT_PENDING_SECTOR_COUNT_R\",\"CURRENT_PENDING_SECTOR_COUNT_R_1\")\ndf_1=df_1.withColumnRenamed(\"POWER_ON_HOURS_R\",\"POWER_ON_HOURS_R_1\")\ndf=df.join(df_1,(((df.ROW) ==  (df_1.ROW_1)) & ((df.SERIAL_NUMBER) ==  (df_1.SERIAL_NUMBER_1))),\"left\")\ndf = df.drop(\"SERIAL_NUMBER_1\")\ndf = df.drop(\"ROW_1\")\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 4.0 Create Max, Min, Sum, Mean and Relative Change Features <a id=\"40\"></a>"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.sql.functions import col\n\nfrom pyspark.sql.functions import greatest\nfrom pyspark.sql.functions import  least\nfrom pyspark.sql.functions import  mean\n# max of last 8\ndf = df.withColumn('POWER_ON_HOURS_R_MAX', greatest('POWER_ON_HOURS_R','POWER_ON_HOURS_R_7','POWER_ON_HOURS_R_6','POWER_ON_HOURS_R_5',\\\n                                                    'POWER_ON_HOURS_R_4','POWER_ON_HOURS_R_3','POWER_ON_HOURS_R_2','POWER_ON_HOURS_R_1'))\n# min of last 8\ndf = df.withColumn('POWER_ON_HOURS_R_MIN', least('POWER_ON_HOURS_R','POWER_ON_HOURS_R_7','POWER_ON_HOURS_R_6','POWER_ON_HOURS_R_5',\\\n                                                    'POWER_ON_HOURS_R_4','POWER_ON_HOURS_R_3','POWER_ON_HOURS_R_2','POWER_ON_HOURS_R_1'))\n# sum of last 8\ndf = df.withColumn('POWER_ON_HOURS_R_SUM', (df.POWER_ON_HOURS_R+df.POWER_ON_HOURS_R_7+df.POWER_ON_HOURS_R_6+df.POWER_ON_HOURS_R_5+\\\n                                                    df.POWER_ON_HOURS_R_4+df.POWER_ON_HOURS_R_3+df.POWER_ON_HOURS_R_2+df.POWER_ON_HOURS_R_1))\n\n#mean of last 8 periods\ndf = df.withColumn('POWER_ON_HOURS_R_MEAN', df.POWER_ON_HOURS_R_SUM/8)\n#variance of last 8\ndf = df.withColumn('POWER_ON_HOURS_R_VAR', df.POWER_ON_HOURS_R_MAX/df.POWER_ON_HOURS_R_MIN)\n#daily variance from running mean\ndf = df.withColumn('POWER_ON_HOURS_R_DELTA', df.POWER_ON_HOURS_R/df.POWER_ON_HOURS_R_MEAN)\n#Running average divided by running max\ndf = df.withColumn('POWER_ON_HOURS_R_VARX', df.POWER_ON_HOURS_R_MEAN/df.POWER_ON_HOURS_R_MAX)\n#runnning average divided by running min\ndf = df.withColumn('POWER_ON_HOURS_R_VARN', df.POWER_ON_HOURS_R_MEAN/df.POWER_ON_HOURS_R_MIN)\n#current valud divided by running max\ndf = df.withColumn('POWER_ON_HOURS_R_DELTAX', df.POWER_ON_HOURS_R/df.POWER_ON_HOURS_R_MAX)\n#current value divided by running min\ndf = df.withColumn('POWER_ON_HOURS_R_DELTAN', df.POWER_ON_HOURS_R/df.POWER_ON_HOURS_R_MIN)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": "df = df.withColumn('POWER_ON_HOURS_N_MAX', greatest('POWER_ON_HOURS_N','POWER_ON_HOURS_N_7','POWER_ON_HOURS_N_6','POWER_ON_HOURS_N_5',\\\n                                                    'POWER_ON_HOURS_N_4','POWER_ON_HOURS_N_3','POWER_ON_HOURS_N_2','POWER_ON_HOURS_N_1'))\n\ndf = df.withColumn('POWER_ON_HOURS_N_MIN', least('POWER_ON_HOURS_N','POWER_ON_HOURS_N_7','POWER_ON_HOURS_N_6','POWER_ON_HOURS_N_5',\\\n                                                    'POWER_ON_HOURS_N_4','POWER_ON_HOURS_N_3','POWER_ON_HOURS_N_2','POWER_ON_HOURS_N_1'))\n\ndf = df.withColumn('POWER_ON_HOURS_N_SUM', (df.POWER_ON_HOURS_N+df.POWER_ON_HOURS_N_7+df.POWER_ON_HOURS_N_6+df.POWER_ON_HOURS_N_5+\\\n                                                    df.POWER_ON_HOURS_N_4+df.POWER_ON_HOURS_N_3+df.POWER_ON_HOURS_N_2+df.POWER_ON_HOURS_N_1))\n\n\ndf = df.withColumn('POWER_ON_HOURS_N_MEAN', df.POWER_ON_HOURS_N_SUM/8)\ndf = df.withColumn('POWER_ON_HOURS_N_VAR', df.POWER_ON_HOURS_N_MAX/df.POWER_ON_HOURS_N_MIN)\ndf = df.withColumn('POWER_ON_HOURS_N_DELTA', df.POWER_ON_HOURS_N/df.POWER_ON_HOURS_N_MEAN)\ndf = df.withColumn('POWER_ON_HOURS_N_VARX', df.POWER_ON_HOURS_N_MEAN/df.POWER_ON_HOURS_N_MAX)\ndf = df.withColumn('POWER_ON_HOURS_N_VARN', df.POWER_ON_HOURS_N_MEAN/df.POWER_ON_HOURS_N_MIN)\ndf = df.withColumn('POWER_ON_HOURS_N_DELTAX', df.POWER_ON_HOURS_N/df.POWER_ON_HOURS_N_MAX)\ndf = df.withColumn('POWER_ON_HOURS_N_DELTAN', df.POWER_ON_HOURS_N/df.POWER_ON_HOURS_N_MIN)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": "df = df.withColumn('REALLOCATED_SECTOR_COUNT_R_MAX', greatest('REALLOCATED_SECTOR_COUNT_R','REALLOCATED_SECTOR_COUNT_R_7','REALLOCATED_SECTOR_COUNT_R_6','REALLOCATED_SECTOR_COUNT_R_5',\\\n                                                    'REALLOCATED_SECTOR_COUNT_R_4','REALLOCATED_SECTOR_COUNT_R_3','REALLOCATED_SECTOR_COUNT_R_2','REALLOCATED_SECTOR_COUNT_R_1'))\n\ndf = df.withColumn('REALLOCATED_SECTOR_COUNT_R_MIN', least('REALLOCATED_SECTOR_COUNT_R','REALLOCATED_SECTOR_COUNT_R_7','REALLOCATED_SECTOR_COUNT_R_6','REALLOCATED_SECTOR_COUNT_R_5',\\\n                                                    'REALLOCATED_SECTOR_COUNT_R_4','REALLOCATED_SECTOR_COUNT_R_3','REALLOCATED_SECTOR_COUNT_R_2','REALLOCATED_SECTOR_COUNT_R_1'))\n\ndf = df.withColumn('REALLOCATED_SECTOR_COUNT_R_SUM', (df.REALLOCATED_SECTOR_COUNT_R+df.REALLOCATED_SECTOR_COUNT_R_7+df.REALLOCATED_SECTOR_COUNT_R_6+df.REALLOCATED_SECTOR_COUNT_R_5+\\\n                                                    df.REALLOCATED_SECTOR_COUNT_R_4+df.REALLOCATED_SECTOR_COUNT_R_3+df.REALLOCATED_SECTOR_COUNT_R_2+df.REALLOCATED_SECTOR_COUNT_R_1))\n\n\ndf = df.withColumn('REALLOCATED_SECTOR_COUNT_R_MEAN', df.REALLOCATED_SECTOR_COUNT_R_SUM/8)\ndf = df.withColumn('REALLOCATED_SECTOR_COUNT_R_VAR', df.REALLOCATED_SECTOR_COUNT_R_MAX/df.REALLOCATED_SECTOR_COUNT_R_MIN)\ndf = df.withColumn('REALLOCATED_SECTOR_COUNT_R_DELTA', df.REALLOCATED_SECTOR_COUNT_R/df.REALLOCATED_SECTOR_COUNT_R_MEAN)\ndf = df.withColumn('REALLOCATED_SECTOR_COUNT_R_VARX', df.REALLOCATED_SECTOR_COUNT_R_MEAN/df.REALLOCATED_SECTOR_COUNT_R_MAX)\ndf = df.withColumn('REALLOCATED_SECTOR_COUNT_R_VARN', df.REALLOCATED_SECTOR_COUNT_R_MEAN/df.REALLOCATED_SECTOR_COUNT_R_MIN)\ndf = df.withColumn('REALLOCATED_SECTOR_COUNT_R_DELTAX', df.REALLOCATED_SECTOR_COUNT_R/df.REALLOCATED_SECTOR_COUNT_R_MAX)\ndf = df.withColumn('REALLOCATED_SECTOR_COUNT_R_DELTAN', df.REALLOCATED_SECTOR_COUNT_R/df.REALLOCATED_SECTOR_COUNT_R_MIN)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": "df = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_N_MAX', greatest('REPORTED_UNCORRECTABLE_ERRORS_N','REPORTED_UNCORRECTABLE_ERRORS_N_7','REPORTED_UNCORRECTABLE_ERRORS_N_6','REPORTED_UNCORRECTABLE_ERRORS_N_5',\\\n                                                    'REPORTED_UNCORRECTABLE_ERRORS_N_4','REPORTED_UNCORRECTABLE_ERRORS_N_3','REPORTED_UNCORRECTABLE_ERRORS_N_2','REPORTED_UNCORRECTABLE_ERRORS_N_1'))\n\ndf = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_N_MIN', least('REPORTED_UNCORRECTABLE_ERRORS_N','REPORTED_UNCORRECTABLE_ERRORS_N_7','REPORTED_UNCORRECTABLE_ERRORS_N_6','REPORTED_UNCORRECTABLE_ERRORS_N_5',\\\n                                                    'REPORTED_UNCORRECTABLE_ERRORS_N_4','REPORTED_UNCORRECTABLE_ERRORS_N_3','REPORTED_UNCORRECTABLE_ERRORS_N_2','REPORTED_UNCORRECTABLE_ERRORS_N_1'))\n\ndf = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_N_SUM', (df.REPORTED_UNCORRECTABLE_ERRORS_N+df.REPORTED_UNCORRECTABLE_ERRORS_N_7+df.REPORTED_UNCORRECTABLE_ERRORS_N_6+df.REPORTED_UNCORRECTABLE_ERRORS_N_5+\\\n                                                    df.REPORTED_UNCORRECTABLE_ERRORS_N_4+df.REPORTED_UNCORRECTABLE_ERRORS_N_3+df.REPORTED_UNCORRECTABLE_ERRORS_N_2+df.REPORTED_UNCORRECTABLE_ERRORS_N_1))\n\n\ndf = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_N_MEAN', df.REPORTED_UNCORRECTABLE_ERRORS_N_SUM/8)\ndf = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_N_VAR', df.REPORTED_UNCORRECTABLE_ERRORS_N_MAX/df.REPORTED_UNCORRECTABLE_ERRORS_N_MIN)\ndf = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_N_DELTA', df.REPORTED_UNCORRECTABLE_ERRORS_N/df.REPORTED_UNCORRECTABLE_ERRORS_N_MEAN)\ndf = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_N_VARX', df.REPORTED_UNCORRECTABLE_ERRORS_N_MEAN/df.REPORTED_UNCORRECTABLE_ERRORS_N_MAX)\ndf = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_N_VARN', df.REPORTED_UNCORRECTABLE_ERRORS_N_MEAN/df.REPORTED_UNCORRECTABLE_ERRORS_N_MIN)\ndf = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_N_DELTAX', df.REPORTED_UNCORRECTABLE_ERRORS_N/df.REPORTED_UNCORRECTABLE_ERRORS_N_MAX)\ndf = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_N_DELTAN', df.REPORTED_UNCORRECTABLE_ERRORS_N/df.REPORTED_UNCORRECTABLE_ERRORS_N_MIN)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": "df = df.withColumn('REALLOCATED_SECTOR_COUNT_N_MAX', greatest('REALLOCATED_SECTOR_COUNT_N','REALLOCATED_SECTOR_COUNT_N_7','REALLOCATED_SECTOR_COUNT_N_6','REALLOCATED_SECTOR_COUNT_N_5',\\\n                                                    'REALLOCATED_SECTOR_COUNT_N_4','REALLOCATED_SECTOR_COUNT_N_3','REALLOCATED_SECTOR_COUNT_N_2','REALLOCATED_SECTOR_COUNT_N_1'))\n\ndf = df.withColumn('REALLOCATED_SECTOR_COUNT_N_MIN', least('REALLOCATED_SECTOR_COUNT_N','REALLOCATED_SECTOR_COUNT_N_7','REALLOCATED_SECTOR_COUNT_N_6','REALLOCATED_SECTOR_COUNT_N_5',\\\n                                                    'REALLOCATED_SECTOR_COUNT_N_4','REALLOCATED_SECTOR_COUNT_N_3','REALLOCATED_SECTOR_COUNT_N_2','REALLOCATED_SECTOR_COUNT_N_1'))\n\ndf = df.withColumn('REALLOCATED_SECTOR_COUNT_N_SUM', (df.REALLOCATED_SECTOR_COUNT_N+df.REALLOCATED_SECTOR_COUNT_N_7+df.REALLOCATED_SECTOR_COUNT_N_6+df.REALLOCATED_SECTOR_COUNT_N_5+\\\n                                                    df.REALLOCATED_SECTOR_COUNT_N_4+df.REALLOCATED_SECTOR_COUNT_N_3+df.REALLOCATED_SECTOR_COUNT_N_2+df.REALLOCATED_SECTOR_COUNT_N_1))\n\n\ndf = df.withColumn('REALLOCATED_SECTOR_COUNT_N_MEAN', df.REALLOCATED_SECTOR_COUNT_N_SUM/8)\ndf = df.withColumn('REALLOCATED_SECTOR_COUNT_N_VAR', df.REALLOCATED_SECTOR_COUNT_N_MAX/df.REALLOCATED_SECTOR_COUNT_N_MIN)\ndf = df.withColumn('REALLOCATED_SECTOR_COUNT_N_DELTA', df.REALLOCATED_SECTOR_COUNT_N/df.REALLOCATED_SECTOR_COUNT_N_MEAN)\ndf = df.withColumn('REALLOCATED_SECTOR_COUNT_N_VARX', df.REALLOCATED_SECTOR_COUNT_N_MEAN/df.REALLOCATED_SECTOR_COUNT_N_MAX)\ndf = df.withColumn('REALLOCATED_SECTOR_COUNT_N_VARN', df.REALLOCATED_SECTOR_COUNT_N_MEAN/df.REALLOCATED_SECTOR_COUNT_N_MIN)\ndf = df.withColumn('REALLOCATED_SECTOR_COUNT_N_DELTAX', df.REALLOCATED_SECTOR_COUNT_N/df.REALLOCATED_SECTOR_COUNT_N_MAX)\ndf = df.withColumn('REALLOCATED_SECTOR_COUNT_N_DELTAN', df.REALLOCATED_SECTOR_COUNT_N/df.REALLOCATED_SECTOR_COUNT_N_MIN)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": "df = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_R_MAX', greatest('REPORTED_UNCORRECTABLE_ERRORS_R','REPORTED_UNCORRECTABLE_ERRORS_R_7','REPORTED_UNCORRECTABLE_ERRORS_R_6','REPORTED_UNCORRECTABLE_ERRORS_R_5',\\\n                                                    'REPORTED_UNCORRECTABLE_ERRORS_R_4','REPORTED_UNCORRECTABLE_ERRORS_R_3','REPORTED_UNCORRECTABLE_ERRORS_R_2','REPORTED_UNCORRECTABLE_ERRORS_R_1'))\n\ndf = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_R_MIN', least('REPORTED_UNCORRECTABLE_ERRORS_R','REPORTED_UNCORRECTABLE_ERRORS_R_7','REPORTED_UNCORRECTABLE_ERRORS_R_6','REPORTED_UNCORRECTABLE_ERRORS_R_5',\\\n                                                    'REPORTED_UNCORRECTABLE_ERRORS_R_4','REPORTED_UNCORRECTABLE_ERRORS_R_3','REPORTED_UNCORRECTABLE_ERRORS_R_2','REPORTED_UNCORRECTABLE_ERRORS_R_1'))\n\ndf = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_R_SUM', (df.REPORTED_UNCORRECTABLE_ERRORS_R+df.REPORTED_UNCORRECTABLE_ERRORS_R_7+df.REPORTED_UNCORRECTABLE_ERRORS_R_6+df.REPORTED_UNCORRECTABLE_ERRORS_R_5+\\\n                                                    df.REPORTED_UNCORRECTABLE_ERRORS_R_4+df.REPORTED_UNCORRECTABLE_ERRORS_R_3+df.REPORTED_UNCORRECTABLE_ERRORS_R_2+df.REPORTED_UNCORRECTABLE_ERRORS_R_1))\n\n\ndf = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_R_MEAN', df.REPORTED_UNCORRECTABLE_ERRORS_R_SUM/8)\ndf = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_R_VAR', df.REPORTED_UNCORRECTABLE_ERRORS_R_MAX/df.REPORTED_UNCORRECTABLE_ERRORS_R_MIN)\ndf = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_R_DELTA', df.REPORTED_UNCORRECTABLE_ERRORS_R/df.REPORTED_UNCORRECTABLE_ERRORS_R_MEAN)\ndf = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_R_VARX', df.REPORTED_UNCORRECTABLE_ERRORS_R_MEAN/df.REPORTED_UNCORRECTABLE_ERRORS_R_MAX)\ndf = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_R_VARN', df.REPORTED_UNCORRECTABLE_ERRORS_R_MEAN/df.REPORTED_UNCORRECTABLE_ERRORS_R_MIN)\ndf = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_R_DELTAX', df.REPORTED_UNCORRECTABLE_ERRORS_R/df.REPORTED_UNCORRECTABLE_ERRORS_R_MAX)\ndf = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_R_DELTAN', df.REPORTED_UNCORRECTABLE_ERRORS_R/df.REPORTED_UNCORRECTABLE_ERRORS_R_MIN)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": "df = df.withColumn('COMMAND_TIMEOUT_R_MAX', greatest('COMMAND_TIMEOUT_R','COMMAND_TIMEOUT_R_7','COMMAND_TIMEOUT_R_6','COMMAND_TIMEOUT_R_5',\\\n                                                    'COMMAND_TIMEOUT_R_4','COMMAND_TIMEOUT_R_3','COMMAND_TIMEOUT_R_2','COMMAND_TIMEOUT_R_1'))\n\ndf = df.withColumn('COMMAND_TIMEOUT_R_MIN', least('COMMAND_TIMEOUT_R','COMMAND_TIMEOUT_R_7','COMMAND_TIMEOUT_R_6','COMMAND_TIMEOUT_R_5',\\\n                                                    'COMMAND_TIMEOUT_R_4','COMMAND_TIMEOUT_R_3','COMMAND_TIMEOUT_R_2','COMMAND_TIMEOUT_R_1'))\n\ndf = df.withColumn('COMMAND_TIMEOUT_R_SUM', (df.COMMAND_TIMEOUT_R+df.COMMAND_TIMEOUT_R_7+df.COMMAND_TIMEOUT_R_6+df.COMMAND_TIMEOUT_R_5+\\\n                                                    df.COMMAND_TIMEOUT_R_4+df.COMMAND_TIMEOUT_R_3+df.COMMAND_TIMEOUT_R_2+df.COMMAND_TIMEOUT_R_1))\n\n\ndf = df.withColumn('COMMAND_TIMEOUT_R_MEAN', df.COMMAND_TIMEOUT_R_SUM/8)\ndf = df.withColumn('COMMAND_TIMEOUT_R_VAR', df.COMMAND_TIMEOUT_R_MAX/df.COMMAND_TIMEOUT_R_MIN)\ndf = df.withColumn('COMMAND_TIMEOUT_R_DELTA', df.COMMAND_TIMEOUT_R/df.COMMAND_TIMEOUT_R_MEAN)\ndf = df.withColumn('COMMAND_TIMEOUT_R_VARX', df.COMMAND_TIMEOUT_R_MEAN/df.COMMAND_TIMEOUT_R_MAX)\ndf = df.withColumn('COMMAND_TIMEOUT_R_VARN', df.COMMAND_TIMEOUT_R_MEAN/df.COMMAND_TIMEOUT_R_MIN)\ndf = df.withColumn('COMMAND_TIMEOUT_R_DELTAX', df.COMMAND_TIMEOUT_R/df.COMMAND_TIMEOUT_R_MAX)\ndf = df.withColumn('COMMAND_TIMEOUT_R_DELTAN', df.COMMAND_TIMEOUT_R/df.COMMAND_TIMEOUT_R_MIN)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": "df = df.withColumn('COMMAND_TIMEOUT_N_MAX', greatest('COMMAND_TIMEOUT_N','COMMAND_TIMEOUT_N_7','COMMAND_TIMEOUT_N_6','COMMAND_TIMEOUT_N_5',\\\n                                                    'COMMAND_TIMEOUT_N_4','COMMAND_TIMEOUT_N_3','COMMAND_TIMEOUT_N_2','COMMAND_TIMEOUT_N_1'))\n\ndf = df.withColumn('COMMAND_TIMEOUT_N_MIN', least('COMMAND_TIMEOUT_N','COMMAND_TIMEOUT_N_7','COMMAND_TIMEOUT_N_6','COMMAND_TIMEOUT_N_5',\\\n                                                    'COMMAND_TIMEOUT_N_4','COMMAND_TIMEOUT_N_3','COMMAND_TIMEOUT_N_2','COMMAND_TIMEOUT_N_1'))\n\ndf = df.withColumn('COMMAND_TIMEOUT_N_SUM', (df.COMMAND_TIMEOUT_N+df.COMMAND_TIMEOUT_N_7+df.COMMAND_TIMEOUT_N_6+df.COMMAND_TIMEOUT_N_5+\\\n                                                    df.COMMAND_TIMEOUT_N_4+df.COMMAND_TIMEOUT_N_3+df.COMMAND_TIMEOUT_N_2+df.COMMAND_TIMEOUT_N_1))\n\n\ndf = df.withColumn('COMMAND_TIMEOUT_N_MEAN', df.COMMAND_TIMEOUT_N_SUM/8)\ndf = df.withColumn('COMMAND_TIMEOUT_N_VAR', df.COMMAND_TIMEOUT_N_MAX/df.COMMAND_TIMEOUT_N_MIN)\ndf = df.withColumn('COMMAND_TIMEOUT_N_DELTA', df.COMMAND_TIMEOUT_N/df.COMMAND_TIMEOUT_N_MEAN)\ndf = df.withColumn('COMMAND_TIMEOUT_N_VARX', df.COMMAND_TIMEOUT_N_MEAN/df.COMMAND_TIMEOUT_N_MAX)\ndf = df.withColumn('COMMAND_TIMEOUT_N_VARN', df.COMMAND_TIMEOUT_N_MEAN/df.COMMAND_TIMEOUT_N_MIN)\ndf = df.withColumn('COMMAND_TIMEOUT_N_DELTAX', df.COMMAND_TIMEOUT_N/df.COMMAND_TIMEOUT_N_MAX)\ndf = df.withColumn('COMMAND_TIMEOUT_N_DELTAN', df.COMMAND_TIMEOUT_N/df.COMMAND_TIMEOUT_N_MIN)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": "df = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_R_MAX', greatest('CURRENT_PENDING_SECTOR_COUNT_R','CURRENT_PENDING_SECTOR_COUNT_R_7','CURRENT_PENDING_SECTOR_COUNT_R_6','CURRENT_PENDING_SECTOR_COUNT_R_5',\\\n                                                    'CURRENT_PENDING_SECTOR_COUNT_R_4','CURRENT_PENDING_SECTOR_COUNT_R_3','CURRENT_PENDING_SECTOR_COUNT_R_2','CURRENT_PENDING_SECTOR_COUNT_R_1'))\n\ndf = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_R_MIN', least('CURRENT_PENDING_SECTOR_COUNT_R','CURRENT_PENDING_SECTOR_COUNT_R_7','CURRENT_PENDING_SECTOR_COUNT_R_6','CURRENT_PENDING_SECTOR_COUNT_R_5',\\\n                                                    'CURRENT_PENDING_SECTOR_COUNT_R_4','CURRENT_PENDING_SECTOR_COUNT_R_3','CURRENT_PENDING_SECTOR_COUNT_R_2','CURRENT_PENDING_SECTOR_COUNT_R_1'))\n\ndf = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_R_SUM', (df.CURRENT_PENDING_SECTOR_COUNT_R+df.CURRENT_PENDING_SECTOR_COUNT_R_7+df.CURRENT_PENDING_SECTOR_COUNT_R_6+df.CURRENT_PENDING_SECTOR_COUNT_R_5+\\\n                                                    df.CURRENT_PENDING_SECTOR_COUNT_R_4+df.CURRENT_PENDING_SECTOR_COUNT_R_3+df.CURRENT_PENDING_SECTOR_COUNT_R_2+df.CURRENT_PENDING_SECTOR_COUNT_R_1))\n\n\ndf = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_R_MEAN', df.CURRENT_PENDING_SECTOR_COUNT_R_SUM/8)\ndf = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_R_VAR', df.CURRENT_PENDING_SECTOR_COUNT_R_MAX/df.CURRENT_PENDING_SECTOR_COUNT_R_MIN)\ndf = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_R_DELTA', df.CURRENT_PENDING_SECTOR_COUNT_R/df.CURRENT_PENDING_SECTOR_COUNT_R_MEAN)\ndf = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_R_VARX', df.CURRENT_PENDING_SECTOR_COUNT_R_MEAN/df.CURRENT_PENDING_SECTOR_COUNT_R_MAX)\ndf = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_R_VARN', df.CURRENT_PENDING_SECTOR_COUNT_R_MEAN/df.CURRENT_PENDING_SECTOR_COUNT_R_MIN)\ndf = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_R_DELTAX', df.CURRENT_PENDING_SECTOR_COUNT_R/df.CURRENT_PENDING_SECTOR_COUNT_R_MAX)\ndf = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_R_DELTAN', df.CURRENT_PENDING_SECTOR_COUNT_R/df.CURRENT_PENDING_SECTOR_COUNT_R_MIN)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": "df = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_N_MAX', greatest('CURRENT_PENDING_SECTOR_COUNT_N','CURRENT_PENDING_SECTOR_COUNT_N_7','CURRENT_PENDING_SECTOR_COUNT_N_6','CURRENT_PENDING_SECTOR_COUNT_N_5',\\\n                                                    'CURRENT_PENDING_SECTOR_COUNT_N_4','CURRENT_PENDING_SECTOR_COUNT_N_3','CURRENT_PENDING_SECTOR_COUNT_N_2','CURRENT_PENDING_SECTOR_COUNT_N_1'))\n\ndf = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_N_MIN', least('CURRENT_PENDING_SECTOR_COUNT_N','CURRENT_PENDING_SECTOR_COUNT_N_7','CURRENT_PENDING_SECTOR_COUNT_N_6','CURRENT_PENDING_SECTOR_COUNT_N_5',\\\n                                                    'CURRENT_PENDING_SECTOR_COUNT_N_4','CURRENT_PENDING_SECTOR_COUNT_N_3','CURRENT_PENDING_SECTOR_COUNT_N_2','CURRENT_PENDING_SECTOR_COUNT_N_1'))\n\ndf = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_N_SUM', (df.CURRENT_PENDING_SECTOR_COUNT_N+df.CURRENT_PENDING_SECTOR_COUNT_N_7+df.CURRENT_PENDING_SECTOR_COUNT_N_6+df.CURRENT_PENDING_SECTOR_COUNT_N_5+\\\n                                                    df.CURRENT_PENDING_SECTOR_COUNT_N_4+df.CURRENT_PENDING_SECTOR_COUNT_N_3+df.CURRENT_PENDING_SECTOR_COUNT_N_2+df.CURRENT_PENDING_SECTOR_COUNT_N_1))\n\n\ndf = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_N_MEAN', df.CURRENT_PENDING_SECTOR_COUNT_N_SUM/8)\ndf = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_N_VAR', df.CURRENT_PENDING_SECTOR_COUNT_N_MAX/df.CURRENT_PENDING_SECTOR_COUNT_N_MIN)\ndf = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_N_DELTA', df.CURRENT_PENDING_SECTOR_COUNT_N/df.CURRENT_PENDING_SECTOR_COUNT_N_MEAN)\ndf = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_N_VARX', df.CURRENT_PENDING_SECTOR_COUNT_N_MEAN/df.CURRENT_PENDING_SECTOR_COUNT_N_MAX)\ndf = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_N_VARN', df.CURRENT_PENDING_SECTOR_COUNT_N_MEAN/df.CURRENT_PENDING_SECTOR_COUNT_N_MIN)\ndf = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_N_DELTAX', df.CURRENT_PENDING_SECTOR_COUNT_N/df.CURRENT_PENDING_SECTOR_COUNT_N_MAX)\ndf = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_N_DELTAN', df.CURRENT_PENDING_SECTOR_COUNT_N/df.CURRENT_PENDING_SECTOR_COUNT_N_MIN)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": "#from pyspark.sql.types import DoubleType\n#from pyspark.sql.functions import col, round\n#df=df.withColumn(\"ROW\",round(df.REALLOCATED_SECTOR_COUNT_R.cast(DoubleType()),2))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Fill missing values wtih -99"
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": "df=df.na.fill(value=-99)"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": "df=df.sort(\"SERIAL_NUMBER\",\"DATE\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 5.0 Append mean encoded data variables <a id=\"50\"></a>"
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": "z='model.csv'\ninput_data = spark.read\\\n    .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n    .option('header', 'true')\\\n    .load(cos.url(z, 'backblazedata-donotdelete-pr-cij57grgkoctem'))#read the file from object storage\ninput_data = input_data.select(\"MODEL\",\"MODEL_FAIL_RATE\",\"MODEL_FAIL_CNT\",\"MODEL_FAIL_TOTAL\",\"GLOBAL_AVG_FAILURE\") #select the relevant fields\ndf_model = reduce(DataFrame,[input_data])#append the current file to the running data frame \n\ndf_model=df_model[['MODEL','MODEL_FAIL_RATE','MODEL_FAIL_CNT','MODEL_FAIL_TOTAL']]"
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": "#df_model = spark.createDataFrame(df_model)\ndf_model=df_model.withColumnRenamed(\"MODEL\",\"MODELM\")"
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": "df=df.join(df_model,(((df.MODEL) ==  (df_model.MODELM)) ),\"left\")\ndf = df.drop(\"MODELM\")"
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": "z='manufacturer.csv'\ninput_data = spark.read\\\n    .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n    .option('header', 'true')\\\n    .load(cos.url(z, 'backblazedata-donotdelete-pr-cij57grgkoctem'))#read the file from object storage\ninput_data = input_data.select(\"MANUFACTURER\",\"MANU_FAIL_RATE\",\"MANU_FAIL_CNT\",\"MANU_FAIL_TOTAL\",\"GLOBAL_AVG_FAILURE\") #select the relevant fields\ndf_manu = reduce(DataFrame,[input_data])#append the current file to the running data frame \n\ndf_manu=df_manu[['MANUFACTURER','MANU_FAIL_RATE','MANU_FAIL_CNT','MANU_FAIL_TOTAL']]\n\n#df_model = spark.createDataFrame(df_model)\ndf_manu=df_manu.withColumnRenamed(\"MANUFACTURER\",\"MANUFACTURERM\")\n\ndf=df.join(df_manu,(((df.MANUFACTURER) ==  (df_manu.MANUFACTURERM)) ),\"left\")\ndf = df.drop(\"MANUFACTURERM\")"
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": "z='df_avg_by_model.csv'\ninput_data = spark.read\\\n    .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n    .option('header', 'true')\\\n    .load(cos.url(z, 'backblazedata-donotdelete-pr-cij57grgkoctem'))#read the file from object storage\ninput_data = input_data.select(\"MODEL\",\"REALLOCATED_SECTOR_COUNT_N_MOD\",\"REPORTED_UNCORRECTABLE_ERRORS_N_MOD\",\"COMMAND_TIMEOUT_N_MOD\",\n                               \"CURRENT_PENDING_SECTOR_COUNT_N_MOD\",\"POWER_ON_HOURS_N_MOD\",\"REALLOCATED_SECTOR_COUNT_R_MOD\",\n                               \"REPORTED_UNCORRECTABLE_ERRORS_R_MOD\",\"COMMAND_TIMEOUT_R_MOD\",\"CURRENT_PENDING_SECTOR_COUNT_R_MOD\",\n                               \"POWER_ON_HOURS_R_MOD\") #select the relevant fields\ndf_model = reduce(DataFrame,[input_data])#append the current file to the running data frame \n\ndf_model=df_model\n#df_model = spark.createDataFrame(df_model)\ndf_model=df_model.withColumnRenamed(\"MODEL\",\"MODELM\")\n\ndf=df.join(df_model,(((df.MODEL) ==  (df_model.MODELM)) ),\"left\")\ndf = df.drop(\"MODELM\")"
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": "z='df_avg_by_manu.csv'\ninput_data = spark.read\\\n    .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n    .option('header', 'true')\\\n    .load(cos.url(z, 'backblazedata-donotdelete-pr-cij57grgkoctem'))#read the file from object storage\ninput_data = input_data.select(\"MANUFACTURER\",\"REALLOCATED_SECTOR_COUNT_N_MAN\",\"REPORTED_UNCORRECTABLE_ERRORS_N_MAN\",\n                               \"COMMAND_TIMEOUT_N_MAN\",\"CURRENT_PENDING_SECTOR_COUNT_N_MAN\",\"POWER_ON_HOURS_N_MAN\",\n                               \"REALLOCATED_SECTOR_COUNT_R_MAN\",\"REPORTED_UNCORRECTABLE_ERRORS_R_MAN\",\"COMMAND_TIMEOUT_R_MAN\",\n                               \"CURRENT_PENDING_SECTOR_COUNT_R_MAN\",\"POWER_ON_HOURS_R_MAN\") #select the relevant fields\ndf_manu = reduce(DataFrame,[input_data])#append the current file to the running data frame \n\ndf_manu=df_manu\n\n#df_model = spark.createDataFrame(df_model)\ndf_manu=df_manu.withColumnRenamed(\"MANUFACTURER\",\"MANUFACTURERM\")\n\ndf=df.join(df_manu,(((df.MANUFACTURER) ==  (df_manu.MANUFACTURERM)) ),\"left\")\ndf = df.drop(\"MANUFACTURERM\")"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Fill in Null values with -99"
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": "df=df.na.fill(value=-99)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 6.0 Create even more features <a id=\"60\"></a>"
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": "\ndf = df.withColumn('REALLOCATED_SECTOR_COUNT_N_MOD_IDX', df.REALLOCATED_SECTOR_COUNT_N/df.REALLOCATED_SECTOR_COUNT_N_MOD)\n\ndf = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_N_MOD_IDX', df.REPORTED_UNCORRECTABLE_ERRORS_N/df.REPORTED_UNCORRECTABLE_ERRORS_N_MOD)\ndf = df.withColumn('COMMAND_TIMEOUT_N_MOD_MOD_IDX', df.COMMAND_TIMEOUT_N/df.COMMAND_TIMEOUT_N_MOD)\n\ndf = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_N_MOD_IDX', df.CURRENT_PENDING_SECTOR_COUNT_N/df.CURRENT_PENDING_SECTOR_COUNT_N_MOD)\n\ndf = df.withColumn('POWER_ON_HOURS_N_MOD_IDX', df.POWER_ON_HOURS_N/df.POWER_ON_HOURS_N_MOD)\ndf = df.withColumn('REALLOCATED_SECTOR_COUNT_R_MOD_IDX', df.REALLOCATED_SECTOR_COUNT_R/df.REALLOCATED_SECTOR_COUNT_R_MOD)\ndf = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_R_MOD_IDX', df.REPORTED_UNCORRECTABLE_ERRORS_R/df.REPORTED_UNCORRECTABLE_ERRORS_R_MOD)\n\ndf = df.withColumn('COMMAND_TIMEOUT_R_MOD_IDX', df.COMMAND_TIMEOUT_R/df.COMMAND_TIMEOUT_R_MOD)\ndf = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_R_MOD_IDX', df.CURRENT_PENDING_SECTOR_COUNT_R/df.CURRENT_PENDING_SECTOR_COUNT_R_MOD)\n\ndf = df.withColumn('POWER_ON_HOURS_R_MOD_IDX', df.POWER_ON_HOURS_R/df.POWER_ON_HOURS_R_MOD)\n"
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": "df = df.withColumn('REALLOCATED_SECTOR_COUNT_N_MAN_IDX', df.REALLOCATED_SECTOR_COUNT_N/df.REALLOCATED_SECTOR_COUNT_N_MAN)\n\ndf = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_N_MAN_IDX', df.REPORTED_UNCORRECTABLE_ERRORS_N/df.REPORTED_UNCORRECTABLE_ERRORS_N_MAN)\ndf = df.withColumn('COMMAND_TIMEOUT_N_MAN_IDX', df.COMMAND_TIMEOUT_N/df.COMMAND_TIMEOUT_N_MAN)\n\ndf = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_N_MAN_IDX', df.CURRENT_PENDING_SECTOR_COUNT_N/df.CURRENT_PENDING_SECTOR_COUNT_N_MAN)\n\ndf = df.withColumn('POWER_ON_HOURS_N_MAN_IDX', df.POWER_ON_HOURS_N/df.POWER_ON_HOURS_N_MAN)\ndf = df.withColumn('REALLOCATED_SECTOR_COUNT_R_MAN_IDX', df.REALLOCATED_SECTOR_COUNT_R/df.REALLOCATED_SECTOR_COUNT_R_MAN)\ndf = df.withColumn('REPORTED_UNCORRECTABLE_ERRORS_R_MAN_IDX', df.REPORTED_UNCORRECTABLE_ERRORS_R/df.REPORTED_UNCORRECTABLE_ERRORS_R_MAN)\n\ndf = df.withColumn('COMMAND_TIMEOUT_R_MAN_IDX', df.COMMAND_TIMEOUT_R/df.COMMAND_TIMEOUT_R_MAN)\ndf = df.withColumn('CURRENT_PENDING_SECTOR_COUNT_R_MAN_IDX', df.CURRENT_PENDING_SECTOR_COUNT_R/df.CURRENT_PENDING_SECTOR_COUNT_R_MAN)\n\ndf = df.withColumn('POWER_ON_HOURS_R_MAN_IDX', df.POWER_ON_HOURS_R/df.POWER_ON_HOURS_R_MAN)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 7.0 Export to Parquet file for step 5 <a id=\"70\"></a>"
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [],
            "source": "df.write.mode(\"overwrite\").parquet(cos.url('data_2020_model.parquet', 'bucketname'))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "All data used in this notebook is the property of BackBlaze.com.\n\nFor questions regarding use of data please see the following website. https://www.backblaze.com/b2/hard-drive-test-data.html"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.9 with Spark",
            "language": "python3",
            "name": "python39"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}