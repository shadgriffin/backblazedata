{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# A Complete Solution to the BackBaze.com Kaggle Problem"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Step One.  \n\n### File Processing"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Table of contents"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "1. [Introduction](#10)<br>\n\n2. [Establish environment and parameters](#20)<br>\n3. [Process Data](#30)<br>\n    3.1 [Create Null Data frame](#31)<br>\n    3.2 [Read in Data from csv files](#32)<br>\n    3.21 [Read in Months with 31 days](#321)<br>\n    3.22 [Read in Months with 30 days](#322)<br>\n    3.23 [Read in Months with 29 days](#323)<br>\n    3.24 [Read in Months with 28 days](#322)<br>\n4. [Prepare Fields ](#40)<br>\n5. [Export to Parquet files](#50)<br>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 1.0 Introduction <a id=\"10\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "BackBaze.com, you are the \"GOAT.\" You are the \"cat's meow.\" You \"Rock the House.\" In case you don't know why BackBaze.com is so totally \"kick-ass,\" they open-sourced a vast set of hard drive information a few years ago and continue updating it each quarter. What a treasure trove of superb data. BackBlaze.com, thank you from the bottom of my heart. \n\nThe backblaze.com data includes operational metrics from hard drives with an indicator of a hard-drive failure. It is an excellent source for teaching techniques related to machine failure. Again, thank you for making this available to the open-source community.\n\nHere is a link to the data.\n\n\nhttps://www.backblaze.com/b2/hard-drive-test-data.html\n\nMy goal in this series of articles is not to give the best solution with the highest AUC.  My goal is to show you how to approach equipment failure problems and build solutions that reflect realistic accuracy, and provide an easy transition from the lab to the real world. \n\nI will use a Spark/Python Jupyter notebook inside IBM's Watson Studio on the cloud as a tool in this discussion.\n\nhttps://www.ibm.com/cloud/watson-studio\n\nI will also be using cloud object storage on the IBM cloud.\n\nhttps://www.ibm.com/cloud/block-storage\n\n\nThe first article in this series is, without question, the most mundane.  I will be loading data from the CSV files provided by backblaze.com into SPARK data frames.  File processing is a tedious but necessary part of the process. "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Before running this code, I downloaded the .csv files from BackBlaze.com and uploaded them to IBM Cloud object storage. Note our friends at backblaze.com conveniently labeled the CSV files in the following format. \n\n\"YYYY_MM_DD.csv.\"\n\n The naming convention makes it easy to load them systematically from cloud object storage to spark data frames. Also, note that I only use a handful of the fields in the CSV file. And finally, I limited my work here to 2018, 2019, and 2020.\n \n I created these notebooks with a runtime useing 1 driver with 1 vCPU and 4 GB RAM, and 2 executors each with 1 vCPU and 4 GB RAM.  This is available for free on the IBM Cloud.  Some of the notebooks take a few hours to run.  You'll need to schedule your notebooks to run as jobs.\n \n https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/schedule-task.html"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 2.0 Establish environment and parameters <a id=\"20\"></a>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Import the Relevant Libraries and connect to object storage."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Import relevant Libraries\nfrom functools import reduce\nfrom pyspark.sql import DataFrame\n\nimport pyspark.sql.functions as F\n\nfrom pyspark.sql.functions import when"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 3.0 Process Data <a id=\"30\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### 3.1 Create Null Data frame <a id=\"31\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "There are many ways to import CSV files into SPARK data frames. I can not say my approach is the best, but I can say with certainty it works. \n\nThe first step is to create a _null_ data frame with the relevant fields and no records. \n\nAgain, I am using only some of the fields available. The schema of the files has changed over time. The fields in the final data appear historically in all files. I also removed columns that did not seem relevant. The point of this exercise is not to build the ultimate model. Instead, it is to demonstrate how I approach these types of problems."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#read in a specific csv file\nbase = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(cos.url('2020-01-02.csv', 'backblazedata-donotdelete-pr-cij57grgkoctem'))\n#Truncate the file so there is no records.\nbase=base.filter(base.serial_number =='DAN PASTORINI')\n#limit the fields to those we are interested in\nbase = base.select(\"date\", \"serial_number\", \"model\",\"capacity_bytes\",\"failure\",\"smart_5_normalized\",\"smart_187_normalized\",\"smart_188_normalized\",\\\n               \"smart_197_normalized\",\"smart_9_normalized\",\"smart_5_raw\",\"smart_187_raw\",\"smart_188_raw\",\\\n               \"smart_197_raw\",\"smart_9_raw\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### 3.2 Read in Data from csv files  <a id=\"32\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In the following blocks of code, I will read each .csv file for 2020, 2019, and 2018.  I created a separate block of code for months based on the number of days in the month.  I process months with 31, 30, 29, and 28 days in separate blocks of code.\n\n\n\nNote that looping through each file one at a time from CSV import to spark minimizes the amount of memory.  The final result is a parque file I can use in the second notebook of this series. \n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "##### 3.21 Read in Months with 31 days.  <a id=\"321\"></a>"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'cos' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;32m/usr/local/share/jupyter/kernels/python39/scripts/launch_ipykernel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m               \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.execution.datasources.csv.CSVFileFormat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m               \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m               \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'backblazedata-donotdelete-pr-cij57grgkoctem'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;31m#select the relevant fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             input_data = input_data.select(\"date\", \"serial_number\", \"model\",\"capacity_bytes\",\"failure\",\"smart_5_normalized\",\"smart_187_normalized\",\"smart_188_normalized\",\\\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'cos' is not defined"
                    ]
                }
            ],
            "source": "# define the years\nfor d in (['2017','2020','2019','2018']):    \n    #define the months with 31 days\n    for t in (['01','03','05','07','08','10','12']):\n        #define the first 9 days -- these will need a leading 0 to identify them\n        for q in range(1,10):  \n            #define the file name based on d, t and q\n            z=(d + \"-\" + t + \"-\" + '0' +str(q)+\".csv\")\n        \n            #read the file from object storage\n            input_data = spark.read\\\n              .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n              .option('header', 'true')\\\n              .load(cos.url(z, 'backblazedata-donotdelete-pr-cij57grgkoctem'))\n            #select the relevant fields\n            input_data = input_data.select(\"date\", \"serial_number\", \"model\",\"capacity_bytes\",\"failure\",\"smart_5_normalized\",\"smart_187_normalized\",\"smart_188_normalized\",\\\n                   \"smart_197_normalized\",\"smart_9_normalized\",\"smart_5_raw\",\"smart_187_raw\",\"smart_188_raw\",\\\n                   \"smart_197_raw\",\"smart_9_raw\")\n            #append the current file to the running data frame originally based on the null frame\n            base = reduce(DataFrame.unionAll, [base,input_data])\n        \n        for q in range(10,32): #define days 10 -31 -- these will NOT need a leading 0 to identify them   \n            z=(d + \"-\" + t + \"-\" +str(q)+\".csv\")#define the file name based on d, t and q\n        \n            #print(z)\n            input_data = spark.read\\\n              .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n              .option('header', 'true')\\\n              .load(cos.url(z, 'backblazedata-donotdelete-pr-cij57grgkoctem'))#read the file from object storage\n            input_data = input_data.select(\"date\", \"serial_number\", \"model\",\"capacity_bytes\",\"failure\",\"smart_5_normalized\",\"smart_187_normalized\",\"smart_188_normalized\",\\\n                   \"smart_197_normalized\",\"smart_9_normalized\",\"smart_5_raw\",\"smart_187_raw\",\"smart_188_raw\",\\\n                   \"smart_197_raw\",\"smart_9_raw\") #select the relevant fields\n            base = reduce(DataFrame.unionAll, [base,input_data])#append the current file to the running data frame "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "##### 3.22 Read in Months with 30 days.  <a id=\"322\"></a>"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "for d in (['2020','2019','2018','2017']):   # define the years\n    for t in (['04','06','09','11']): #define the months with 30 days\n        for q in range(1,10): #define the first 9 days -- these will need a leading 0 to identify them   \n            z=(d + \"-\" + t + \"-\" + '0' +str(q)+\".csv\")#define the file name based on d, t and q\n        \n            #print(z)\n            input_data = spark.read\\\n              .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n              .option('header', 'true')\\\n              .load(cos.url(z, 'backblazedata-donotdelete-pr-cij57grgkoctem'))#read the file from object storage\n            input_data = input_data.select(\"date\", \"serial_number\", \"model\",\"capacity_bytes\",\"failure\",\"smart_5_normalized\",\"smart_187_normalized\",\"smart_188_normalized\",\\\n                   \"smart_197_normalized\",\"smart_9_normalized\",\"smart_5_raw\",\"smart_187_raw\",\"smart_188_raw\",\\\n                   \"smart_197_raw\",\"smart_9_raw\")#select the relevant fields\n            base = reduce(DataFrame.unionAll, [base,input_data]) #append the current file to the running data frame\n        \n        for q in range(10,31): #define days 10 - 30 -- these will NOT need a leading 0 to identify them     \n            z=(d + \"-\" + t + \"-\" +str(q)+\".csv\")#define the file name based on d, t and q\n        \n            #print(z)\n            input_data = spark.read\\\n              .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n              .option('header', 'true')\\\n              .load(cos.url(z, 'backblazedata-donotdelete-pr-cij57grgkoctem'))#read the file from object storage\n            input_data = input_data.select(\"date\", \"serial_number\", \"model\",\"capacity_bytes\",\"failure\",\"smart_5_normalized\",\"smart_187_normalized\",\"smart_188_normalized\",\\\n                   \"smart_197_normalized\",\"smart_9_normalized\",\"smart_5_raw\",\"smart_187_raw\",\"smart_188_raw\",\\\n                   \"smart_197_raw\",\"smart_9_raw\")#select the relevant fields\n            base = reduce(DataFrame.unionAll, [base,input_data])#append the current file to the running data frame "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "##### 3.23 Read in Months with 29 days.  <a id=\"323\"></a>"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "for d in (['2020']):    # define the years  \n    for t in (['02']):#define the months with 29 days\n        for q in range(1,10):  #define the first 9 days -- these will need a leading 0 to identify them    \n            z=(d + \"-\" + t + \"-\" + '0' +str(q)+\".csv\")#define the file name based on d, t and q\n        \n            #print(q)\n            #print(z)\n            input_data = spark.read\\\n              .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n              .option('header', 'true')\\\n              .load(cos.url(z, 'backblazedata-donotdelete-pr-cij57grgkoctem'))#read the file from object storage\n            input_data = input_data.select(\"date\", \"serial_number\", \"model\",\"capacity_bytes\",\"failure\",\"smart_5_normalized\",\"smart_187_normalized\",\"smart_188_normalized\",\\\n                   \"smart_197_normalized\",\"smart_9_normalized\",\"smart_5_raw\",\"smart_187_raw\",\"smart_188_raw\",\\\n                   \"smart_197_raw\",\"smart_9_raw\")#select the relevant fields\n            base = reduce(DataFrame.unionAll, [base,input_data])#append the current file to the running data frame\n        \n        for q in range(10,30):   #define days 10 -29 -- these will NOT need a leading 0 to identify them   \n            z=(d + \"-\"  + t + \"-\" +str(q)+\".csv\")#define the file name based on d, t and q\n        \n            #print(q)\n            #print(z)\n            input_data = spark.read\\\n              .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n              .option('header', 'true')\\\n              .load(cos.url(z, 'backblazedata-donotdelete-pr-cij57grgkoctem'))#read the file from object storage\n            input_data = input_data.select(\"date\", \"serial_number\", \"model\",\"capacity_bytes\",\"failure\",\"smart_5_normalized\",\"smart_187_normalized\",\"smart_188_normalized\",\\\n                   \"smart_197_normalized\",\"smart_9_normalized\",\"smart_5_raw\",\"smart_187_raw\",\"smart_188_raw\",\\\n                   \"smart_197_raw\",\"smart_9_raw\")#select the relevant fields\n            base = reduce(DataFrame.unionAll, [base,input_data])#append the current file to the running data frame "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "##### 3.24 Read in Months with 28 days.  <a id=\"324\"></a>"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "for d in (['2019','2018','2017']):    # define the years   \n    for t in (['02']):#define the months with 28 days \n        for q in range(1,10):  #define the first 9 days -- these will need a leading 0 to identify them \n            z=(d + \"-\" + t + \"-\" + '0' +str(q)+\".csv\") #define the file name based on d, t and q\n        \n            #print(q)\n            #print(z)\n            input_data = spark.read\\\n              .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n              .option('header', 'true')\\\n              .load(cos.url(z, 'backblazedata-donotdelete-pr-cij57grgkoctem'))#read the file from object storage\n            input_data = input_data.select(\"date\", \"serial_number\", \"model\",\"capacity_bytes\",\"failure\",\"smart_5_normalized\",\"smart_187_normalized\",\"smart_188_normalized\",\\\n                   \"smart_197_normalized\",\"smart_9_normalized\",\"smart_5_raw\",\"smart_187_raw\",\"smart_188_raw\",\\\n                   \"smart_197_raw\",\"smart_9_raw\")#select the relevant fields\n            base = reduce(DataFrame.unionAll, [base,input_data])#append the current file to the running data frame\n        \n        for q in range(10,29):  #define days 10 - 28 -- these will NOT need a leading 0 to identify them   \n            z=(d + \"-\"  + t + \"-\" +str(q)+\".csv\")#define the file name based on d, t and q\n        \n            #print(q)\n            #print(z)\n            input_data = spark.read\\\n              .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n              .option('header', 'true')\\\n              .load(cos.url(z, 'backblazedata-donotdelete-pr-cij57grgkoctem'))#read the file from object storage\n            input_data = input_data.select(\"date\", \"serial_number\", \"model\",\"capacity_bytes\",\"failure\",\"smart_5_normalized\",\"smart_187_normalized\",\"smart_188_normalized\",\\\n                   \"smart_197_normalized\",\"smart_9_normalized\",\"smart_5_raw\",\"smart_187_raw\",\"smart_188_raw\",\\\n                   \"smart_197_raw\",\"smart_9_raw\")#select the relevant fields\n            base = reduce(DataFrame.unionAll, [base,input_data])#append the current file to the running data frame"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 4.0 Prepare Fields <a id=\"40\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In the next section I clean up the fields.\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# rename the dataframe for simplicity\ndf=base\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Give the fields descriptive names"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# rename each field so it is more descriptive\ndf = df.withColumnRenamed(\"smart_5_normalized\",\"REAllOCATED_SECTOR_COUNT_N\") \\\n    .withColumnRenamed(\"smart_187_normalized\",\"REPORTED_UNCORRECTABLE_ERRORS_N\")\\\n    .withColumnRenamed(\"smart_188_normalized\",\"COMMAND_TIMEOUT_N\")\\\n    .withColumnRenamed(\"smart_197_normalized\",\"CURRENT_PENDING_SECTOR_COUNT_N\")\\\n    .withColumnRenamed(\"smart_198_normalized\",\"OFFLINE_UNCORRECTABLE_N\")\\\n    .withColumnRenamed(\"smart_9_normalized\",\"POWER_ON_HOURS_N\")\\\n    .withColumnRenamed(\"smart_5_raw\",\"REAllOCATED_SECTOR_COUNT_R\")\\\n    .withColumnRenamed(\"smart_187_raw\",\"REPORTED_UNCORRECTABLE_ERRORS_R\")\\\n    .withColumnRenamed(\"smart_188_raw\",\"COMMAND_TIMEOUT_R\")\\\n    .withColumnRenamed(\"smart_197_raw\",\"CURRENT_PENDING_SECTOR_COUNT_R\")\\\n    .withColumnRenamed(\"smart_198_raw\",\"OFFLINE_UNCORRECTABLE_R\")\\\n    .withColumnRenamed(\"smart_9_raw\",\"POWER_ON_HOURS_R\")\\\n    .withColumnRenamed(\"date\",\"DATE\")\\\n    .withColumnRenamed(\"serial_number\",\"SERIAL_NUMBER\")\\\n    .withColumnRenamed(\"model\",\"MODEL\")\\\n    .withColumnRenamed(\"capacity_bytes\",\"CAPACITY_BYTES\")\\\n    .withColumnRenamed(\"failure\",\"FAILURE\").cache()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The model field needs a little cleaning up.  The Seagate brand, for example, has 5 different values in the data. We will create a new field called \"Manufacturer\" to correct this."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "\n\ndf = df.withColumn('MANUFACTURER',when (F.instr(df.MODEL, 'TOSHIBA') > 0,'TOSHIBA')\\\n                   .when(F.instr(df.MODEL, 'SG') > 0,'SEAGATE')\\\n                   .when(F.instr(df.MODEL, 'ST') > 0,'SEAGATE')\\\n                   .when(F.instr(df.MODEL, 'Sea') > 0,'SEAGATE')\\\n                   .when(F.instr(df.MODEL, 'SG') > 0,'SEAGATE')\\\n                   .when(F.instr(df.MODEL, 'HGST') > 0,'HGST')\\\n                   .when(F.instr(df.MODEL, 'WD') > 0,'WD')\\\n                   .when(F.instr(df.MODEL, 'DELL') > 0,'DELL')\\\n                   .when(F.instr(df.MODEL, 'Hit') > 0,'HITACHI').otherwise('pp')).cache()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ""
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 5.0 Export to Parquet files <a id=\"50\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "As we progress through this exercise, I will use the 2018 and 2019 to create mean encoded dummy variables and I will use the 2020 to build my model.  Given that each year in the available data has a different function, I will create two different dataframes.  One, for 2020 and another for 2018 and 2019.\n\nFor more info on mean encoded dummy variables, please see the following article.\n\nhttps://medium.com/towards-data-science/leveraging-value-from-postal-codes-naics-codes-area-codes-and-other-funky-arse-categorical-be9ce75b6d5a"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df_2020=df.filter(df.DATE>='2020-01-01').cache()\ndf_2019_2017=df.filter(df.DATE<='2019-12-31').cache()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now I will export these data frames to a parque file that we can use in subsequent steps."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Write 2020 data to parquet file."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#cos.url(filenametowrite,bucketnameforyourproject). For example:\n\ndf_2020.write.mode(\"overwrite\").parquet(cos.url('data2020.parquet', 'backblazedata-donotdelete-pr-cij57grgkoctem'))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Write 2019, 2018 and 2017 data to a parquet file."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#cos.url(filenametowrite,bucketnameforyourproject). For example:\n\ndf_2019_2017.write.mode(\"overwrite\").parquet(cos.url('data2019_2017.parquet', 'backblazedata-donotdelete-pr-cij57grgkoctem'))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "All data used in this notebook is the property of BackBlaze.com. \n\nFor questions regarding use of data please see the following website.\nhttps://www.backblaze.com/b2/hard-drive-test-data.html"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.9 with Spark",
            "language": "python3",
            "name": "python39"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}